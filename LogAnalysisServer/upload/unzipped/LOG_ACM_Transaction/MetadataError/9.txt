{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/11008","fieldValue":"Xin Liu"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/11008","fieldValue":" Motivated by the increasing popularity of learning and predicting human user behavior in communication and computing systems, in this paper, we investigate the fundamental benefit of predictive scheduling, i.e., predicting and pre-serving arrivals, in controlled queueing systems. Based on a lookahead-window prediction model, we first establish a novel queue-equivalence between the predictive queueing system with a fully efficient scheduling scheme and an equivalent queueing system without prediction. This result allows us to analytically demonstrate that predictive scheduling necessarily improves system delay performance and drives it to zero with increasing prediction power. It also enables us to exactly determine the required prediction power for different systems and study its impact on tail delay. We then propose the Predictive Backpressure PBP algorithm for achieving optimal utility performance in such predictive systems. PBP efficiently incorporates prediction into stochastic system control and avoids the great complication due to the exponential state space growth in the prediction window size. We show that PBP achieves a utility performance that is within Oε of the optimal, for any ε > 0, while guaranteeing that the system delay distribution is a shifted-to-the-left version of that under the original Backpressure algorithm. Hence, the average delay under PBP is strictly better than that under Backpressure, and vanishes with increasing prediction window size. This implies that the resulting utility-delay tradeoff with predictive scheduling can beat the known optimal [Oε,Olog1\/ε] tradeoff for systems without prediction. We also develop the Predictable-Only PBP POPBP algorithm and show that it effectively reduces packet delay in systems where traffic can only be predicted but not pre-served."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/11008","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/11008","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/11008","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/2006","fieldValue":"Alvarez-Mesa, Mauricio"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/2006","fieldValue":" Temporal SIMT (TSIMT) has been suggested as an alternative to conventional (spatial) SIMT for improving GPU performance on branch-intensive code. Although TSIMT has been briefly mentioned before, it was not evaluated. We present a complete design and evaluation of TSIMT GPUs, along with the inclusion of scalarization and a combination of temporal and spatial SIMT, named Spatiotemporal SIMT (STSIMT). Simulations show that TSIMT alone results in a performance reduction, but a combination of scalarization and STSIMT yields a mean performance enhancement of 19.6&percnt; and improves the energy-delay product by 26.2&percnt; compared to SIMT."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/2006","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/2006","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/11009","fieldValue":" Modern access network technologies like Long Term Evolution LTE and High Speed Packet Access HSPA use time-slotting mechanisms to optimize resource sharing and overall network performance. In time-slotted networks, the one-way delay of all packets in a packet stream depends on the absolute point in time when the first packet of the stream is sent. With appropriate feedback signals, applications can exploit this effect to reduce their effective end-to-end delay. Time-critical applications such as real-time sensor data acquisition or voice-over-IP VoIP communications can shift their acquisition interval in order to adapt to the network timing. Information about the actual time-slotting periods can be gathered by active network measurements or through implementation of cross-layer information exchange. In this paper, a method is proposed to determine the optimum send time for particular destinations and to support applications in adjusting their send time accordingly. Theoretical findings are supported by the offline analysis of measurement data and by a proof-of-concept implementation that confirms the feasibility and effectiveness of the proposed solution in operational LTE and HSPA networks."}