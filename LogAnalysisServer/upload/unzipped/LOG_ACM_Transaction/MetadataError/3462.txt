{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/21331","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/21331","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/21332","fieldValue":" Two important measures of performance for the surveillance applications of the mobile sensor networks are detection latency and system lifetime. Previous work on modeling detection delay has assumed that sensor measurements are delivered to the fusion center with zero delay. Such approaches can require excessive energy, resulting into reduced lifetime. This article argues that a trade-off between detection latency and system lifetime can be made by employing an energy aware transmission scheme. The article formulates the trade-off as an optimization problem, and presents an analytic method to model both detection latency and system lifetime. The model is substantiated by using simulation."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/21332","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/21332","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/21333","fieldValue":" We consider two related data gathering problems for wireless sensor networks (WSNs). The MLDA problem is concerned with maximizing the system lifetime T so that we can perform T rounds of data gathering with in-network aggregation, given the initial available energy of the sensors. The $M^2EDA problem is concerned with minimizing the maximum energy consumed by any one sensor when performing T rounds of data gathering with in-network aggregation, for a given T. We provide an effective algorithm for finding an everywhere sparse integral solution to the M2EDA problem which is within a factor of Î± &equals; 1+ 4n\/T of the optimum, where n is the number of nodes. A solution is everywhere sparse if the number of communication links for any subset X of nodes is O(X), in our case at most 4&vert;X&vert;. Since often T &equals; Ï\u2030(n), we obtain the first everywhere sparse, asymptotically optimal integral solutions to the M2EDA problem. Everywhere sparse solutions are desirable since then almost all sensors have small number of incident communication links and small overhead for maintaining state. We also show that the MLDA and M2EDA problems are essentially equivalent, in the sense that we can obtain an optimal fractional solution to an instance of the MLDA problem by scaling an optimal fractional solution to a suitable instance of the M2$EDA problem. As a result, our algorithm is effective at finding everywhere sparse, asymptotically optimal, integral solutions to the MLDA problem, when the initial available energy of the sensors is sufficient for supporting optimal system lifetime which is Ï\u2030(n)."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/21333","fieldValue":"Everywhere sparse approximately optimal minimum energy data gathering and aggregation in sensor networks"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/21333","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/21333","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3033","fieldValue":" This article proposes a new distortion model for phrase-based statistical machine translation. In decoding, a distortion model estimates the source word position to be translated next (subsequent position; SP) given the last translated source word position (current position; CP). We propose a distortion model that can simultaneously consider the word at the CP, the word at an SP candidate, the context of the CP and an SP candidate, relative word order among the SP candidates, and the words between the CP and an SP candidate. These considered elements are called rich context. Our model considers rich context by discriminating label sequences that specify spans from the CP to each SP candidate. It enables our model to learn the effect of relative word order among SP candidates as well as to learn the effect of distances from the training data. In contrast to the learning strategy of existing methods, our learning strategy is that the model learns preference relations among SP candidates in each sentence of the training data. This leaning strategy enables consideration of all of the rich context simultaneously. In our experiments, our model had higher BLUE and RIBES scores for Japanese-English, Chinese-English, and German-English translation compared to the lexical reordering models."}