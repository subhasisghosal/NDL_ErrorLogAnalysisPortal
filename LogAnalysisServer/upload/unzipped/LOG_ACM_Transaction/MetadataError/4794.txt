{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25422","fieldValue":" Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being the duration of the smallest note in the music, as opposed to fixed length segmentation in spoken language recognition. It is found that acoustic features extracted from the rhythm based segmentation scheme outperform those from fixed length segmentation. We also study the effectiveness of a musically motivated acoustic feature. Octave scale cepstral coefficients (OSCCs) by comparing with the other acoustic features: Log frequency cepstral coefficients, Linear prediction coefficients (LPC) and LPC-derived cepstral coefficients. Finally, we examine the modeling capabilities of Gaussian mixture models and support vector machines in sung language recognition experiments. Experiments conducted on a corpus of 400 popular songs sung in English, Chinese, German, and Indonesian, showed that the OSCC feature outperforms other features. A sung language recognition accuracy of 64.9&percnt; was achieved when Gaussian mixture models were trained on shifted-delta-OSCC acoustic features, extracted via rhythm based music segmentation."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25422","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25422","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25423","fieldValue":" This article introduces the notion of virtual feature stream, a feature stream defined from a primary data stream, in which at any time only the features that are needed to compute the queries that are currently running in the system are computed. Virtual feature streams are, in general, impossible to determine a priori, but the paper introduces an algorithm that stops the computation of features as soon as it can be proved that they are no longer needed thus generating, albeit in a roundabout and more expensive than the ideal way, a feature stream that is less expensive than the complete one to compute and safe: the queries that accept the virtual feature stream are those (and only those) that would accept the original feature stream."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25423","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25423","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25424","fieldValue":" Interactive Filmmaking is both an aesthetic and technological challenge. Steerable plots, where audiences are not passive viewers but active participants of the narrative experience, require an engaging narrative model as well as a technologically feasible structure. This article discusses the connection between aesthetics, cinema, and interactivity and presents a model for interactive narration that is based on the audience's ability to read and interpret footage differently according to its context. Through a detour narrative model it is possible to engage audiences in a coconstructive hypermedia experience while at the same time minimizing the amount of footage required. An interface model that allows seamless hypervideo navigation through graphic interaction is also discussed, and the interactive short film The Crime or Revenge of Fernando Moreno is presented, along with user experience and usability studies that experimentally prove our hypothesis."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25424","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25424","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3455","fieldValue":" Virtual reality systems commonly include both monocular and binocular depth cues, which have the potential to provide viewers with a realistic impression of spatial properties of the virtual environment. However, when multiple viewers share the same display, only one viewer typically receives the projectively correct images. All other viewers experience the same images despite displacement from the center of projection (CoP). Three experiments evaluated perceptual distortions caused by displacement from the CoP and compared those percepts to predictions of models based on monocular and binocular viewing geometry. Leftward and rightward displacement from the CoP caused virtual angles on the ground plane to be judged as larger and smaller, respectively, compared to judgments from the CoP. Backward and forward displacement caused rectangles on the ground plane to be judged as larger and smaller in depth, respectively, compared to judgments from the CoP. Judgment biases were in the same direction as cue-based model predictions but of smaller magnitude. Displacement from the CoP had asymmetric effects on perceptual judgments, unlike model predictions. Perceptual distortion occurred with monocular cues alone but was exaggerated when binocular cues were added. The results are grounded in terms of practical implications for multiuser virtual environments."}