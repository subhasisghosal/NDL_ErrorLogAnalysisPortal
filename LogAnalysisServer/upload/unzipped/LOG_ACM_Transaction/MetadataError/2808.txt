{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19269","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/19270","fieldValue":" Finding a feasible point that satisfies a set of constraints is a common task in scientific computing; examples are the linear feasibility problem and the convex feasibility problem. Finitely convergent sequential algorithms can be used for solving such problems; an example of such an algorithm is ART3, which is defined in such a way that its control is cyclic in the sense that during its execution it repeatedly cycles through the given constraints. Previously we found a variant of ART3 whose control is no longer cyclic, but which is still finitely convergent and in practice usually converges faster than ART3. In this article we propose a general methodology for automatic transformation of finitely convergent sequential algorithms in such a way that (1) finite convergence is retained, and (2) the speed of convergence is improved. The first of these properties is proven by mathematical theorems, the second is illustrated by applying the algorithms to a practical problem."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/19270","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19270","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/19271","fieldValue":" This note offers a new approach based on a least squares fit to past data in order to select the stepsize when solving an ordinary differential equation. The approach used may have applicability to other situations where one wants to repeatedly make short term predictions given somewhat noisy data. Additional ad hoc rules help significantly for reliability and efficiency. Comparisons with some Runge-Kutta codes, an Adams code, and an extrapolation code are also included."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19271","fieldValue":"ODE"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/19271","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19271","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/19272","fieldValue":"Van Eekelen, Marko"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/19272","fieldValue":" On w-bit processors which are much faster at multiplying two w-bit integers than at dividing 2w-bit integers by w-bit integers, reductions of large integers by moduli M smaller than $2^w-1 are often implemented suboptimally, leading applications to take excessive processing time. We present a modular reduction algorithm implementing division by a modulus through multiplication by a reciprocal of that modulus, a well-known method for moduli larger than 2w-1. We show that application of this method to smaller moduli makes it possible to express certain modular sums and differences without having to compensate for word overflows. By embedding the algorithm in a loop and applying a few transformations to the loop, we obtain an algorithm for reduction of large integers by moduli up to 2w-1. Implementations of this algorithm can run considerably faster than implementations of similar algorithms that allow for moduli up to 2w$. This is substantiated by measurements on processors with relatively fast multiplication instructions. It is notoriously hard to specify efficient mathematical algorithms on the level of abstract machine instructions in an error-free manner. In order to eliminate the chance of errors as much as possible, we have created formal correctness proofs of our algorithms, checked by a mechanized proof assistant."}