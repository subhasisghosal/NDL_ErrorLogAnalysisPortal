{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/6740","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6740","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/6741","fieldValue":" Remarkable progress has been made in the past few decades in various aspects of radiation therapy (RT). However, some of these promising technologies, such as image-guided online replanning and arc therapy, rely heavily on the availability of fast dose calculation. In this article, based on a popular dose calculation algorithm, the Collapsed-Cone Convolution\/Superposition (CCCS) algorithm, we present a multi-FPGA accelerator to speed up radiation dose calculation. Our performance-driven design strategy yields a fully pipelined architecture, which includes a resource-economic raytracing engine and high-performance energy deposition pipeline. An evaluation based on a set of clinical treatment planning cases confirms that our FPGA design almost fully utilizes the available external memory bandwidth and achieves close to the best possible performance for the CCCS algorithm while using less resource. Compared with an existing FPGA design which aimed to accelerate the identical algorithm, the proposed design achieved 1.9X speedup by providing better memory bandwidth utilization (81.7&percnt; v.s. 43&percnt; of the available external memory bandwidth), higher working frequency (90MHz v.s. 70MHz) and less logic resource usage (25K v.s. 55K logic cells). Furthermore, it obtains a speedup of 20X over a commercial multithreaded software on a quad-core system and 15X performance improvement over closely related results. In terms of accuracy, the measured less than 1&percnt; statistical fluctuation indicates that our solution is practical in real medical scenarios."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/6741","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6741","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/6742","fieldValue":" There have been both research and commercial advances on applying Wireless Sensor and Actuator Networks (WSN) in industrial premises. These have cost advantages related to avoiding some cabled deployments. A possible architecture involves a Networked Control System (NCS) with many small WSN subnetworks, cabled nodes and computer servers (e.g., servers, control stations). In those systems individual sensor nodes can be programmed, as opposed to cabled analog systems. We investigate approaches for networked-wide configuration, where all nodesâ\u20AC\u201Dcabled or WSN sensorsâ\u20AC\u201Dcan be configured with simplicity from a single interface, instead of hand-coding or complex configurations of individual nodes. We propose an architecture and approach for configuration and operation. Previous related proposals on middleware involving WSNs suffer from two major limitations: they either program within an individual WSN or configure operation outside WSNs, wrapping data coming from WSN. They do not allow configuring WSN and non-WSN nodes for operation from a single interface. We discuss the architecture and propose the NCSWSN configuration and operation approach. We are applying this system in an industrial testbed, therefore we test the approach and also show user interfaces and results from the deployment."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/6742","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6742","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/6743","fieldValue":" Process variability and dynamic domains increase the uncertainty of embedded systems and force designers to apply pessimistic designs, which become unnecessarily conservative and have a tremendous impact on both performance and energy consumption. In this context, developing uncertainty-aware design methodologies that take both variation at platform and at application level into account becomes a must. These methodologies should mitigate the effects derived from uncertainty, avoiding worst-case assumptions. In this article we propose a comprehensive methodology to tackle two forms of uncertainty: (1) process variation on the memory system, (2) application dynamism. A statistical model has been developed to deal with variability derived from fabrication process, whereas system scenarios are selected to cope with dynamic domains. Both sources of uncertainty are firstly tackled in combination at design time, to be refined later, at setup. As a result, at run time the platform can be successfully adapted to the current application behaviour as well as the current variations. Our simulations show that this methodology provides significant energy savings while still meeting strict timing constraints."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/6743","fieldValue":"System-level memory management based on statistical variability compensation for frame-based applications"}