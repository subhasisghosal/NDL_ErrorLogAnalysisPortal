{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25079","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25080","fieldValue":"Lo, Shih-Hsiang"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25080","fieldValue":"Lee, Che-Rung"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25080","fieldValue":"Chung, I-Hsin"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25080","fieldValue":"Chung, Yeh-Ching"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25080","fieldValue":" Box intersection checking is a common task used in many large-scale simulations. Traditional methods cannot provide fast box intersection checking with large-scale datasets. This article presents a parallel algorithm to perform Pairwise Box Intersection checking on Graphics processing units (PBIG). The PBIG algorithm consists of three phases: planning, mapping and checking. The planning phase partitions the space into small cells, the sizes of which are determined to optimize performance. The mapping phase maps the boxes into the cells. The checking phase examines the box intersections in the same cell. Several performance optimizations, including load-balancing, output data compression\/encoding, and pipelined execution, are presented for the PBIG algorithm. The experimental results show that the PBIG algorithm can process large-scale datasets and outperforms three well-performing algorithms."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25080","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25080","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25081","fieldValue":"Glynn, Peter W"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25081","fieldValue":" Consider a simulation estimator Î±(c) based on expending c units of computer time to estimate a quantity Î±. In comparing competing estimators for Î±, a natural figure of merit is to choose the estimator that minimizes the computation time needed to reduce the error probability P(|Î±(c)â\u20AC\u2030âˆ\u2019â\u20AC\u2030Î±|â\u20AC\u2030>â\u20AC\u2030Îµ) to below some prescribed value Î\u201D. In this paper, we develop large deviations results that provide approximations to the computational budget necessary to reduce the error probability to below Î\u201D when Î\u201D is small. This approximation depends critically on both the distribution of the estimator itself and that of the random amount of computer time required to generate the estimator, and leads to different conclusions regarding the choice of preferred estimator than those obtained when one requires the error tolerance Îµ to be small. The â\u20ACœsmall Îµâ\u20AC? regime leads to variance-based selection criteria, and has a long history in the simulation literature going back to Hammersley and Handscomb."}