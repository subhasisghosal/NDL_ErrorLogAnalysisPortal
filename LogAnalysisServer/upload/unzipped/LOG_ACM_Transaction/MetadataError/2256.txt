{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/17487","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/17487","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/17488","fieldValue":" In correlation-based time-of-flight (C-ToF) imaging systems, light sources with temporally varying intensities illuminate the scene. Due to global illumination, the temporally varying radiance received at the sensor is a combination of light received along multiple paths. Recovering scene properties (e.g., scene depths) from the received radiance requires separating these contributions, which is challenging due to the complexity of global illumination and the additional temporal dimension of the radiance. We propose phasor imaging, a framework for performing fast inverse light transport analysis using C-ToF sensors. Phasor imaging is based on the idea that, by representing light transport quantities as phasors and light transport events as phasor transformations, light transport analysis can be simplified in the temporal frequency domain. We study the effect of temporal illumination frequencies on light transport and show that, for a broad range of scenes, global radiance (inter-reflections and volumetric scattering) vanishes for frequencies higher than a scene-dependent threshold. We use this observation for developing two novel scene recovery techniques. First, we present micro-ToF imaging, a ToF-based shape recovery technique that is robust to errors due to inter-reflections (multipath interference) and volumetric scattering. Second, we present a technique for separating the direct and global components of radiance. Both techniques require capturing as few as 3--4 images and minimal computations. We demonstrate the validity of the presented techniques via simulations and experiments performed with our hardware prototype."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/17488","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/17488","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/17489","fieldValue":"Tsai, Yu-Ting"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/17489","fieldValue":" This article presents a generalized sparse multilinear model, namely multiway K-clustered tensor approximation (MK-CTA), for synthesizing photorealistic 3D images from large-scale multidimensional visual datasets. MK-CTA extends previous tensor approximation algorithms, particularly K-clustered tensor approximation (K-CTA) [Tsai and Shih 2012], to partition a multidimensional dataset along more than one dimension into overlapped clusters. On the contrary, K-CTA only sparsely clusters a dataset along just one dimension and often fails to efficiently approximate other unclustered dimensions. By generalizing K-CTA with multiway sparse clustering, MK-CTA can be regarded as a novel sparse tensor-based model that simultaneously exploits the intra- and inter-cluster coherence among different dimensions of an input dataset. Our experiments demonstrate that MK-CTA can accurately and compactly represent various multidimensional datasets with complex and sharp visual features, including bidirectional texture functions (BTFs) [Dana et al. 1999], time-varying light fields (TVLFs) [Bando et al. 2013], and time-varying volume data (TVVD) [Wang et al. 2010], while easily achieving high rendering rates in practical graphics applications."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/17489","fieldValue":"Multiway K-Clustered Tensor Approximation: Toward High-Performance Photorealistic Data-Driven Rendering"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/17489","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/17489","fieldValue":"ACM"}