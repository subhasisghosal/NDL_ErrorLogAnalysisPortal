{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10619","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10619","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10620","fieldValue":" As network link rates are being pushed beyond 40 Gb\/s, IP lookup in high-speed routers is moving to hardware. The ternary content addressable memory (TCAM)-based IP lookup engine and the static random access memory (SRAM)-based IP lookup pipeline are the two most common ways to achieve high throughput. However, route updates in both engines degrade lookup performance and may lead to packet drops. Moreover, there is a growing interest in virtual IP routers where more frequent updates happen. Finding solutions that achieve both fast lookup and low update overhead becomes critical. In this paper, we propose a hybrid IP lookup architecture to address this challenge. The architecture is based on an efficient trie partitioning scheme that divides the forwarding information base (FIB) into two prefix sets: a large disjoint leaf prefix set mapped into an external TCAM-based lookup engine and a small overlapping prefix set mapped into an on-chip SRAM-based lookup pipeline. Critical optimizations are developed on both IP lookup engines to reduce the update overhead. We show how to extend the proposed hybrid architecture to support virtual routers. Our implementation shows a throughput of 250 million lookups per second (equivalent to 128 Gb\/s with 64-B packets). The update overhead is significantly lower than that of previous work, the memory consumption is reasonable, and the utilization ratio of most external TCAMs is up to 100%."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/10620","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10620","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10620","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10621","fieldValue":" A complete flow statistics report should include both flow size (the number of packets in a flow) counting and flow volume (the number of bytes in a flow) counting. Although previous studies have contributed a lot to the flow size counting problem, it is still a great challenge to well support the flow volume statistics due to the demanding requirements on both memory size and memory bandwidth in monitoring device. In this paper, we propose a DIScount COunting (DISCO) method, which is designed for both flow size and flow bytes counting. For each incoming packet of length l, DISCO increases the corresponding counter assigned to the flow with an increment that is less than l. With an elaborate design on the counter update rule and the inverse estimation, DISCO saves memory consumption while providing an accurate unbiased estimator. The method is evaluated thoroughly under theoretical analysis and simulations with synthetic and real traces. The results demonstrate that DISCO is more accurate than related work given the same counter sizes. DISCO is also implemented on the network processor Intel IXP2850 for a performance test. Using only one microengine (ME) in IXP2850, the throughput can reach up to 11.1 Gb\/s under a traditional traffic pattern. The throughput increases to 39 Gb\/s when employing four MEs."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/10621","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10621","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10621","fieldValue":"ACM"}