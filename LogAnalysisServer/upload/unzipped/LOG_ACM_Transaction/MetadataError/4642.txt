{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24998","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24998","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/24999","fieldValue":" We extend and analyze a new class of estimators for the variance parameter of a steady-state simulation output process. These estimators are based on ‚\u20ACúfolded‚\u20AC? versions of the standardized time series (STS) of the process, and are analogous to the area and Cram√©r--von Mises estimators calculated from the original STS. In fact, one can apply the folding mechanism more than once to produce an entire class of estimators, all of which reuse the same underlying data stream. We show that these folded estimators share many of the same properties as their nonfolded counterparts, with the added bonus that they are often nearly independent of the nonfolded versions. In particular, we derive the asymptotic distributional properties of the various estimators as the run length increases, as well as their bias, variance, and mean squared error. We also study linear combinations of these estimators, and we show that such combinations yield estimators with lower variance than their constituents. Finally, we consider the consequences of batching, and we see that the batched versions of the new estimators compare favorably to benchmark estimators such as the nonoverlapping batch means estimator."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24999","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24999","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25000","fieldValue":" In this article, we develop a stochastic approximation method to solve a monotone estimation problem and use this method to enhance the empirical performance of the Q-learning algorithm when applied to Markov decision problems with monotone value functions. We begin by considering a monotone estimation problem where we want to estimate the expectation of a random vector, Œ∑. We assume that the components of E {Œ∑} are known to be in increasing order. The stochastic approximation method that we propose is designed to exploit this information by projecting its iterates onto the set of vectors with increasing components. The novel aspect of the method is that it uses projections with respect to the max norm. We show the almost sure convergence of the stochastic approximation method. After this result, we consider the Q-learning algorithm when applied to Markov decision problems with monotone value functions. We study a variant of the Q-learning algorithm that uses projections to ensure that the value function approximation obtained at each iteration is also monotone. Computational results indicate that the performance of the Q-learning algorithm can be improved significantly by exploiting the monotonicity property of the value functions."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/25000","fieldValue":"A stochastic approximation method with max-norm projections and its applications to the Q-learning algorithm"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25000","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25000","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25001","fieldValue":"Kim, Seong-Hee"}