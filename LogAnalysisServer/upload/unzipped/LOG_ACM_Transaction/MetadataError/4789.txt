{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25409","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25410","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25410","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25411","fieldValue":"Lin, Yu-Ching"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25411","fieldValue":"Yang, Yi-Hsuan"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25411","fieldValue":" The online repository of music tags provides a rich source of semantic descriptions useful for training emotion-based music classifier. However, the imbalance of the online tags affects the performance of emotion classification. In this paper, we present a novel data-sampling method that eliminates the imbalance but still takes the prior probability of each emotion class into account. In addition, a two-layer emotion classification structure is proposed to harness the genre information available in the online repository of music tags. We show that genre-based grouping as a precursor greatly improves the performance of emotion classification. On the average, the incorporation of online genre tags improves the performance of emotion classification by a factor of 55&percnt; over the conventional single-layer system. The performance of our algorithm for classifying 183 emotion classes reaches 0.36 in example-based f-score."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25411","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25411","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25412","fieldValue":" Photos are a special way to tell stories of our best memories and moments. The representation of those photos in appealing physical photo books is highly appreciated by many people. Today, many photos are shared via social networking sites, where people upload their photos and share their stories with their friends. The members of social networks comment on each other's photos, add tags or descriptions and upload new photos of the same events to their albums. While the media of different personal events are available on the social network, there is no easy way to collect and bundle them into a story and print this story as a photo book. We propose an approach to automatically detect media elements that match a query (where, when, what, who) in the user's social network and intelligently arrange and compose them into a printable photo book. We combine content analysis of text and images to automatically and semi-automatically select photos of a specific story. We calculate the probabilities of each two photos to belong to the same event using an Expectation-Maximization algorithm that we propose in order to be able to retrieve them easily when receiving the user queries, and we address the differences between our model and other models that use similar proposed algorithms. People's tags and the interaction between the users and the photos as well as other semantic information are exploited to select important photos that are suitable to create the photo book. The selected photos and derived semantics are then employed to automatically create an appealing layout for the photo book."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25412","fieldValue":"ACM"}