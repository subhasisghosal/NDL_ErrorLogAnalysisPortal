{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25778","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25779","fieldValue":" Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload\/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystalâ\u20AC™s content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25779","fieldValue":"CDN"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25779","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25779","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3487","fieldValue":" In this article, we identify adaptive sampling strategies for haptic signals. Our approach relies on experiments wherein we record the response of several users to haptic stimuli. We then learn different classifiers to predict the user response based on a variety of causal signal features. The classifiers that have good prediction accuracy serve as candidates to be used in adaptive sampling. We compare the resultant adaptive samplers based on their rate-distortion tradeoff using synthetic as well as natural data. For our experiments, we use a haptic device with a maximum force level of 3 N and 10 users. Each user is subjected to several piecewise constant haptic signals and is required to click a button whenever he perceives a change in the signal. For classification, we not only use classifiers based on level crossings and Weberâ\u20AC™s law but also random forests using a variety of causal signal features. The random forest typically yields the best prediction accuracy and a study of the importance of variables suggests that the level crossings and Weberâ\u20AC™s classifier features are most dominant. The classifiers based on level crossings and Weberâ\u20AC™s law have good accuracy (more than 90&percnt;) and are only marginally inferior to random forests. The level crossings classifier consistently outperforms the one based on Weberâ\u20AC™s law even though the gap is small. Given their simple parametric form, the level crossings and Weberâ\u20AC™s law--based classifiers are good candidates to be used for adaptive sampling. We study their rate-distortion performance and find that the level crossing sampler is superior. For example, for haptic signals obtained while exploring various rendered objects, for an average sampling rate of 10 samples per second, the level crossings adaptive sampler has a mean square error about 3dB less than the Weber sampler."}{"fieldName":"dc.subject","informationCode":"ERR_FOUND_HTML_TAG","handle":"12345678_acm\/3487","fieldValue":"Weber&rsquo;s law"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3487","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3487","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25780","fieldValue":" The demand for multimedia streaming over mobile networks has been steadily increasing over the past several years. For instance, it has become common for mobile users to stream full TV episodes, sports events, and movies while on the go. Unfortunately, this growth in demand has strained the wireless networks despite the significant increase of their capacities with recent generations. Hence, efficient utilization of the expensive and limited wireless spectrum remains an important problem, especially in the context of multimedia streaming services that consume a large portion of the bandwidth capacity. In this article, we introduce the idea of dynamically configuring cells in wireless cellular networks to form single-frequency networks based on the multimedia traffic demands from users in each cell. We formulate the resource allocation problem in such complex networks with the goal of maximizing the number of served multimedia streams, and we prove that this problem is NP-Complete. Then we present an optimal solution to maximize the number of served multimedia streams within a cellular network. This optimal solution, however, may suffer from an exponential time complexity in the worst case, which is not practical for real-time streaming over large-scale networks. Therefore, we propose a heuristic algorithm with polynomial running time to provide faster and more practical solution for real-time deployments. Through detailed packet-level simulations, we assess the performance of the proposed algorithms with respect to the average service ratio, energy saving, video quality, frame loss rate, initial buffering time, rate of re-buffering events, and bandwidth overhead. We show that the proposed algorithms achieve substantial improvements in all of these performance metrics compared to the state-of-the-art approaches. For example, for the service ratio metric, our algorithms can serve up to 11 times more users compared to the unicast approach, and they achieve up to 54% improvement over the closest multicast approaches in the literature."}