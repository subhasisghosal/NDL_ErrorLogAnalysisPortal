{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5871","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5871","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/5872","fieldValue":"Liu, Leslie S"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5872","fieldValue":" Studies have shown positive impact of video blogs (vlogs) on patient education. However, we know little on how patient-initiated vlogs shape the relationships among vloggers and viewers. We qualitatively analyzed 72 vlogs on YouTube by users diagnosed with HIV, diabetes, or cancer and 1,274 comments posted to the vlogs to understand viewersâ\u20AC™ perspectives on the vlogs. We found that the unique video medium allowed intense and enriched personal and contextual disclosure to the viewers, leading to strong community-building activities and social support among vloggers and commenters, both informationally and emotionally. Furthermore, the unique communication structure of the vlogs allowed ad hoc small groups to form, which showed different group behavior than typical text-based social media, such as online communities. We provide implications to the Health Care Industry (HCI) community on how future technologies for health vlogs could be designed to further support chronic illness management."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5872","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5872","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5873","fieldValue":" Touch-sensitive surfaces have become a predominant input medium for computing devices. In particular, multitouch capability of these devices has given rise to developing rich interaction vocabularies for â\u20ACœrealâ\u20AC? direct manipulation of user interfaces. However, the richness and flexibility of touch interaction often comes with significant complexity for programming these behaviors. Particularly, finger touches, though intuitive, are imprecise and lead to ambiguity. Touch input often involves coordinated movements of multiple fingers as opposed to the single pointer of a traditional WIMP interface. It is challenging in not only detecting the intended motion carried out by these fingers but also in determining the target objects being manipulated due to multiple focus points. Currently, developers often need to build touch behaviors by dealing with raw touch events that is effort consuming and error-prone. In this article, we present Touch, a tool that allows developers to easily specify their desired touch behaviors by demonstrating them live on a touch-sensitive device or selecting them from a list of common behaviors. Developers can then integrate these touch behaviors into their application as resources and via an API exposed by our runtime framework. The integrated tool support enables developers to think and program optimistically about how these touch interactions should behave, without worrying about underlying complexity and technical details in detecting target behaviors and invoking application logic. We discuss the design of several novel inference algorithms that underlie these tool supports and evaluate them against a multitouch dataset that we collected from end users. We also demonstrate the usefulness of our system via an example application."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5873","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5873","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1494","fieldValue":" Increases in the prevalence of dementia and Alzheimerâ\u20AC™s disease (AD) are a growing challenge in many nations where healthcare infrastructures are ill-prepared for the upcoming demand for personal caregiving. To help individuals with AD live at home for longer, we are developing a mobile robot, called ED, intended to assist with activities of daily living through visual monitoring and verbal prompts in cases of difficulty. In a series of experiments, we study speech-based interactions between ED and each of 10 older adults with AD as the latter complete daily tasks in a simulated home environment. Traditional automatic speech recognition is evaluated in this environment, along with rates of verbal behaviors that indicate confusion or trouble with the conversation. Analysis reveals that speech recognition remains a challenge in this setup, especially during household tasks with individuals with AD. Across the verbal behaviors that indicate confusion, older adults with AD are very likely to simply ignore the robot, which accounts for over 40&percnt; of all such behaviors when interacting with the robot. This work provides a baseline assessment of the types of technical and communicative challenges that will need to be overcome for robots to be used effectively in the home for speech-based assistance with daily living."}