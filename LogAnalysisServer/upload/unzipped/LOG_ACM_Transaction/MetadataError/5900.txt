{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5664","fieldValue":" We present a study of a mobile mixed reality game called Can You See Me Now? in which online players are chased through a virtual model of a city by â\u20AC˜runnersâ\u20AC™ (professional performers equipped with GPS and WiFi technologies) who have to run through the actual city streets in order to catch the players. We present an ethnographic study of the game as it toured through two different cities and draws upon video recordings of online players, runners, technical support crew, and also on system logs of text communication. Our study reveals the diverse ways in which online players experienced the uncertainties inherent in GPS and WiFi, including being mostly unaware of them, but sometimes seeing them as problems, or treating the as a designed feature of the game, and even occasionally exploiting them within gameplay. In contrast, the runners and technical crew were fully aware of these uncertainties and continually battled against them through an ongoing and distributed process of orchestration. As a result, we encourage designers to deal with such uncertainties as a fundamental characteristic of location-based experiences rather than treating them as exceptions or bugs that might be ironed out in the future. We argue that designers should explicitly consider four potential states of being of a mobile participant: connected and tracked, connected but not tracked, tracked but not connected, and neither connected nor tracked. We then introduce five strategies that might be used to deal with uncertainty in these different states for different kinds of participant: remove it, hide it, manage it, reveal it, and exploit it. Finally, we present proposals for new orchestration interfaces that reveal the â\u20AC˜seamsâ\u20AC™ in the underlying technical infrastructure by visualizing the recent performance of GPS and WiFi and predicting the likely future performance of GPS."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5664","fieldValue":"GPS"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5664","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5664","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5665","fieldValue":" Most people do not often read privacy policies because they tend to be long and difficult to understand. The Platform for Privacy Preferences (P3P) addresses this problem by providing a standard machine-readable format for website privacy policies. P3P user agents can fetch P3P privacy policies automatically, compare them with a user's privacy preferences, and alert and advise the user. Developing user interfaces for P3P user agents is challenging for several reasons: privacy policies are complex, user privacy preferences are often complex and nuanced, users tend to have little experience articulating their privacy preferences, users are generally unfamiliar with much of the terminology used by privacy experts, users often do not understand the privacy-related consequences of their behavior, and users have differing expectations about the type and extent of privacy policy information they would like to see. We developed a P3P user agent called Privacy Bird. Our design was informed by privacy surveys and our previous experience with prototype P3P user agents. We describe our design approach, compare it with the approach used in other P3P use agents, evaluate our design, and make recommendations to designers of other privacy agents."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5665","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5665","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5666","fieldValue":" In order to investigate large information spaces effectively, it is often necessary to employ navigation mechanisms that allow users to view information at different scales. Some tasks require frequent movements and scale changes to search for details and compare them. We present a model that makes predictions about user performance on such comparison tasks with different interface options. A critical factor embodied in this model is the limited capacity of visual working memory, allowing for the cost of visits via fixating eye movements to be compared to the cost of visits that require user interaction with the mouse. This model is tested with an experiment that compares a zooming user interface with a multi-window interface for a multiscale pattern matching task. The results closely matched predictions in task performance times; however error rates were much higher with zooming than with multiple windows. We hypothesized that subjects made more visits in the multi-window condition, and ran a second experiment using an eye tracker to record the pattern of fixations. This revealed that subjects made far more visits back and forth between pattern locations when able to use eye movements than they made with the zooming interface. The results suggest that only a single graphical object was held in visual working memory for comparisons mediated by eye movements, reducing errors by reducing the load on visual working memory. Finally we propose a design heuristic: extra windows are needed when visual comparisons must be made involving patterns of a greater complexity than can be held in visual working memory."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5666","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5666","fieldValue":"ACM"}