{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/4022","fieldValue":" In this paper, an adaptive time-frequency analysis scheme is proposed along with a synthesis scheme using an asymmetric window. The proposed scheme is suitable for audio noise reduction with a low delay in the range of 0 to 4 ms. The main novelty of the paper is the adaptive analysis scheme that can adapt to the incoming signal independently in both time and frequency by employing a complex filter on a DFT modulated filter bank. A number of adaptive time-frequency schemes are described that are suitable for low delay and low computational complexity. The adaptive time-frequency scheme is used for the computation of noise reduction gain factors, which are then adopted in a nonadaptive analysis\/synthesis scheme. The synthesis scheme uses an asymmetric window to achieve a good tradeoff between low delay and a sharp frequency response. Examples are given of the adaptive analysis and measurements of the synthesis scheme are given to show that the filter bank has a gain dependent nonlinear phase response. Finally, a noise reduction task is performed that shows good performance compared to reference implementations in terms of segmental SNR and PESQ."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/4022","fieldValue":"{\"doi\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/4022","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/4022","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/4023","fieldValue":"Wang, Zhong-Qiu"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/4023","fieldValue":"Wang, DeLiang"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/4023","fieldValue":" Robustness against noise and reverberation is critical for ASR systems deployed in real-world environments. In robust ASR, corrupted speech is normally enhanced using speech separation or enhancement algorithms before recognition. This paper presents a novel joint training framework for speech separation and recognition. The key idea is to concatenate a deep neural network (DNN) based speech separation frontend and a DNN-based acoustic model to build a larger neural network, and jointly adjust the weights in each module. This way, the separation fron-tend is able to provide enhanced speech desired by the acoustic model and the acoustic model can guide the separation frontend to produce more discriminative enhancement. In addition, we apply sequence training to the jointly trained DNN so that the linguistic information contained in the acoustic and language models can be back-propagated to influence the separation frontend at the training stage. To further improve the robustness, we add more noise- and reverberation-robust features for acoustic modeling. At the test stage, utterance-level unsupervised adaptation is performed to adapt the jointly trained network by learning a linear transformation of the input of the separation frontend. The resulting sequence-discriminative jointly-trained multistream system with run-time adaptation achieves 10.63% average word error rate (WER) on the test set of the reverberant and noisy CHiME-2 dataset (task-2), which represents the best performance on this dataset and a 22.75% error reduction over the best existing method."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/4023","fieldValue":"{\"doi\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/4023","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/4023","fieldValue":"ACM"}