{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24735","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/24736","fieldValue":" In order to make formal and analytic models more realistic, overheads which were previously ignored or vastly simplified must be included. We consider the feasibility of characterizing the overheads in conservative asynchronous simulations for such models, and we focus on a single communication structure (i.e., meshes) and use both multicomputer programs and a queueing network as example applications. We find that the two most important issues for modeling are to understand how to estimate the time spent in sending null messages and how to account for the resulting overhead due to the input waiting rule. For null messages, we estimate both the number of messages sent as well as the cost per null message. The number of messages sent can often be estimated by the application, although irregularities in communication structure and edge effects in communication can affect these estimates. A constant is valid as a first-order approximation for the time per null message, but there are secondary factors that one may wish to model, such as load balancing and communication irregularities. The overhead attributable to the input waiting rule depends on several factors: communication structure, communication frequency, and processor load balancing. Irregularity in any of these dimensions can adversely affect the performance of the conservative strategy. It appears feasible to use the factors contributing to the overheads (i.e. context switch costs; null-message costs; percentage of looping due to the conservative synchronization) in a formal model to estimate the cost of the conservative overheads."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24736","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24736","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/24737","fieldValue":" There are many ways for users to share the radio spectrum allocated to a cell in a cellular phone system. We analyze a commonly proposed scheme wh ere the cell is divided into s sectors. Each sector has exclusive access to a certain number of channels. The remaining channels reside in a â\u20ACœcommon poolâ\u20AC? and are shared among the sectors. The smallest unit of bandwidth that can be borrowed from the common pool is a â\u20ACœcarrier,â\u20AC? which consists of c channels. When viewed as a multidimensional birth-death process, the steady-state distribution of the number of active channels in each sector has a â\u20ACœproduct form,â\u20AC? but because the state space is large and has a nonlinear boundary, direct calculation of quantities of interest is usually impractical. Ross and Wang have developed a Monte-Carlo technique that applies to our problem. We significantly improve the efficiency of their technique when applied to our problem by including certain (nonlinear) control variates. The kinds of control variates we use can be applied to other loss systems as well. We also explore the effect of importance sampling for our system. In many cases the variance reduction achieved from the combination of importance sampling and control variates is far greater than from either method alone. For systems with blocking probabilities in the range 0.001 to 0.1, the variance of the system-blocking probability estimator can be reduced by several orders of magnitude."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/24737","fieldValue":"Efficient Monte-Carlo simulation of a product-form model for a cellular system with dynamic resource sharing"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24737","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24737","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3390","fieldValue":" In a virtual environment (VE), efficient techniques are often needed to economize on rendering computation without compromising the information transmitted. The reported experiments devise a functional fidelity metric by exploiting research on memory schemata. According to the proposed measure, similar information would be transmitted across synthetic and real-world scenes depicting a specific schema. This would ultimately indicate which areas in a VE could be rendered in lower quality without affecting information uptake. We examine whether computationally more expensive scenes of greater visual fidelity affect memory performance after exposure to immersive VEs, or whether they are merely more aesthetically pleasing than their diminished visual quality counterparts. Results indicate that memory schemata function in VEs similar to real-world environments. â\u20ACœHigh-levelâ\u20AC? visual cognition related to late visual processing is unaffected by ubiquitous graphics manipulations such as polygon count and depth of shadow rendering; â\u20ACœnormalâ\u20AC? cognition operates as long as the scenes look acceptably realistic. However, when the overall realism of the scene is greatly reduced, such as in wireframe, then visual cognition becomes abnormal. Effects that distinguish schema-consistent from schema-inconsistent objects change because the whole scene now looks incongruent. We have shown that this effect is not due to a failure of basic recognition."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3390","fieldValue":"ACM"}