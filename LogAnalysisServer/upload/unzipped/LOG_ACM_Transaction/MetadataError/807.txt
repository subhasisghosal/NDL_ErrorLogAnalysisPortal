{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/13377","fieldValue":"Chen, Hsin-Hung"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/13377","fieldValue":"Yang, Dau-Jieu"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/13377","fieldValue":"Chang, Hsung-Pin"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/13377","fieldValue":" Increasing the degree of parallelism and reducing the overhead of garbage collection (GC overhead) are the two keys to enhancing the performance of solid-state drives (SSDs). SSDs employ multichannel architectures, and a data placement scheme in an SSD determines how the data are striped to the channels. Without considering the data access pattern, existing fixed and device-level data placement schemes may have either high GC overhead or poor I\/O parallelism, resulting in degraded performance. In this article, an adaptive block-level data placement scheme called BLAS is proposed to maximize the I\/O parallelism while simultaneously minimizing the GC overhead. In contrast to existing device-level schemes, BLAS allows different data placement policies for blocks with different access patterns. Pages in read-intensive blocks are scattered over various channels to maximize the degree of read parallelism, while pages in each of the remaining blocks are attempted to be gathered in the same physical block to minimize the GC overhead. Moreover, BLAS allows the placement policy for a logical block to be changed dynamically according to the access pattern changes of that block. Finally, a parallelism-aware write buffer management approach is adopted in BLAS to maximize the degree of write parallelism. Performance results show that BLAS yields a significant improvement in the SSD response time when compared to existing device-level schemes. In particular, BLAS outperforms device-level page striping and device-level block striping by factors of up to 8.75 and 7.41, respectively. Moreover, BLAS achieves low GC overhead and is effective in adapting to workload changes."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/13377","fieldValue":"SSD"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/13377","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/13377","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/13378","fieldValue":"Bathen, Luis Angel D"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/13378","fieldValue":" The era of cloud computing on-a-chip is enabled by the aggressive move towards many-core platforms and the rapid adoption of Network-on-Chips. As a result, there is a need for large-scale distributed on-chip shared memories that are reliable, low power, and seamlessly manageable. In this work, we propose SPMCloud, a novel scratchpad-memory-based cloud-inspired volatile storage subsystem designed to meet the needs of future-generation many-core platforms. SPMCloud is composed of several concepts, including: (1) a highly scalable data-center-like memory subsystem that exploits two enterprise-network-inspired memory configurations, namely, embedded Network Attached Storage (eNAS) and embedded Storage Area Network (eSAN), and (2) on-demand allocation of reliable memory space through memory virtualization and the use of embedded RAIDs. Our experimental results on Mediabench\/CHStone benchmarks show that the SPMCloud's fully distributed reliable memory subsystems can achieve 48&percnt; energy savings and 70&percnt; latency reduction on average over state-of-the-art NoC memory reliability techniques. We then evaluate the scalability of the SPMCloud and compare it with traditional SPM allocation policies. The SPMCloud's dynamic allocator outperforms the best competition by an average 60&percnt; (eNAS) and 46&percnt; (eSAN) when the platform runs at 250 MHz and by an average 80&percnt; (eNAS) and 40&percnt; when running at 1 GHz. Moreover, the SPMCloud achieves an average 83&percnt; energy savings across all configurations (number of cores) with respect to the best competitors when running at 250 MHz and 1 GHz. We then studied the SPM hit ratio across the various allocation policies discussed in this article and showed that on average the SPMCloud's priority-driven dynamic allocation policy achieves 93.5&percnt; SPM hit ratio, 0.6&percnt; higher hit ratio than the closest allocation policy. We then showed that the eNAS and eSAN achieve an average of 67.9&percnt; and 29&percnt; reduction in execution time, respectively, over the best competitor. Similarly, the eNAS and eSAN achieve an average of 82.7&percnt; and 82.3&percnt; energy savings, respectively, over the best competitor. Furthermore, we evaluated the scalability of the SPMCloud and its performance\/energy efficiency when providing support for some of the heavier E-RAID levels, and showed that the eNAS\/eSAN configurations with SECDED achieve an average of 51.5&percnt; and 34.9&percnt; reduction in execution time, respectively, over the best competitor with SECDED. Similarly, the eNAS\/eSAN configurations with E-RAID Level 1, &plus; SECDED achieve an average of 82.3&percnt; and 75.6&percnt; energy savings, respectively, over the best competitor."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/13378","fieldValue":"ACM"}