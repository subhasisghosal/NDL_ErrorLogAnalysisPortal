{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/21039","fieldValue":"Narasimha Reddy, A L"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/21039","fieldValue":" Modern computer systems have been built around the assumption that persistent storage is accessed via a slow, block-based interface. However, emerging nonvolatile memory technologies (sometimes referred to as storage class memory (SCM)), are poised to revolutionize storage systems. The SCM devices can be attached directly to the memory bus and offer fast, fine-grained access to persistent storage. In this article, we propose a new file system---SCMFS, which is specially designed for Storage Class Memory. SCMFS is implemented on the virtual address space and utilizes the existing memory management module of the operating system to help mange the file system space. As a result, we largely simplified the file system operations of SCMFS, which allowed us a better exploration of performance gain from SCM. We have implemented a prototype in Linux and evaluated its performance through multiple benchmarks. The experimental results show that SCMFS outperforms other memory resident file systems, tmpfs, ramfs and ext2 on ramdisk, and achieves about 70&percnt; of memory bandwidth for file read\/write operations."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/21039","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/21039","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/21040","fieldValue":" Online, remote, data replication is critical for todayâ\u20AC™s enterprise IT organization. Availability of data is key to the success of the organization. A few hours of downtime can cost from thousands to millions of dollars With increasing frequency, companies are instituting disaster recovery plans to ensure appropriate data availability in the event of a catastrophic failure or disaster that destroys a site (e.g. flood, fire, or earthquake). Synchronous and asynchronous replication technologies have been available for a long period of time. Synchronous replication has the advantage of no data loss, but due to latency, synchronous replication is limited by distance and bandwidth. Asynchronous replication on the other hand has no distance limitation, but leads to some data loss which is proportional to the data lag. We present a novel method, implemented within EMC Recover-Point, which allows the system to dynamically move between these replication options without any disruption to the I\/O path. As latency grows, the system will move from synchronous replication to semi-synchronous replication and then to snapshot shipping. It returns to synchronous replication as more bandwidth is available and latency allows."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/21040","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/21040","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/21041","fieldValue":" BTRFS is a Linux filesystem that has been adopted as the default filesystem in some popular versions of Linux. It is based on copy-on-write, allowing for efficient snapshots and clones. It uses B-trees as its main on-disk data structure. The design goal is to work well for many use cases and workloads. To this end, much effort has been directed to maintaining even performance as the filesystem ages, rather than trying to support a particular narrow benchmark use-case. Linux filesystems are installed on smartphones as well as enterprise servers. This entails challenges on many different fronts. ---Scalability. The filesystem must scale in many dimensions: disk space, memory, and CPUs. ---Data integrity. Losing data is not an option, and much effort is expended to safeguard the content. This includes checksums, metadata duplication, and RAID support built into the filesystem. ---Disk diversity. The system should work well with SSDs and hard disks. It is also expected to be able to use an array of different sized disks, which poses challenges to the RAID and striping mechanisms. This article describes the core ideas, data structures, and algorithms of this filesystem. It sheds light on the challenges posed by defragmentation in the presence of snapshots, and the tradeoffs required to maintain even performance in the face of a wide spectrum of workloads."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/21041","fieldValue":"RAID"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/21041","fieldValue":"ACM"}