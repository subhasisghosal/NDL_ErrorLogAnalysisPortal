{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10409","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10409","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1947","fieldValue":" ARM ISA-based processors are no longer low-cost, low-power processors. Nowadays, ARM ISA-based processor manufacturers are striving to implement medium-end to high-end processor cores, which implies implementing a state-of-the-art out-of-order execution engine. Unfortunately, providing efficient out-of-order execution on legacy ARM codes may be quite challenging due to guarded instructions. Predicting the guarded instructions addresses the main serialization impact associated with guarded instructions execution and the multiple definition problem. Moreover, guard prediction allows one to use a global branch-and-guard history predictor to predict both branches and guards, often improving branch prediction accuracy. Unfortunately, such a global branch-and-guard history predictor requires the systematic use of guard predictions. In that case, poor guard prediction accuracy would lead to poor overall performance on some applications. Building on top of recent advances in branch prediction and confidence estimation, we propose a hybrid branch-and-guard predictor, combining a global branch history component and global branch-and-guard history component. The potential gain or loss due to the systematic use of guard prediction is dynamically evaluated at runtime. Two computing modes are enabled: systematic guard prediction use and high-confidence-only guard prediction use. Our experiments show that on most applications, an overwhelming majority of guarded instructions are predicted. Therefore, a simple but relatively inefficient hardware solution can be used to execute the few unpredicted guarded instructions. Significant performance benefits are observed on most applications, while applications with poorly predictable guards do not suffer from performance loss."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1947","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/1947","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10410","fieldValue":" Decision tree-based packet classification algorithms are easy to implement and allow the tradeoff between storage and throughput. However, the memory consumption of these algorithms remains quite high when high throughput is required. The Adaptive Binary Cuttings (ABC) algorithm exploits another degree of freedom to make the decision tree adapt to the geometric distribution of the filters. The three variations of the adaptive cutting procedure produce a set of different-sized cuts at each decision step, with the goal to balance the distribution of filters and to reduce the filter duplication effect. The ABC algorithm uses stronger and more straightforward criteria for decision tree construction. Coupled with an efficient node encoding scheme, it enables a smaller, shorter, and well-balanced decision tree. The hardware-oriented implementation of each variation is proposed and evaluated extensively to demonstrate its scalability and sensitivity to different configurations. The results show that the ABC algorithm significantly outperforms the other decision tree-based algorithms. It can sustain more than 10-Gb\/s throughput and is the only algorithm among the existing well-known packet classification algorithms that can compete with TCAMs in terms of the storage efficiency."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/10410","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10410","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10410","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10411","fieldValue":" Multicast\/broadcast is regarded as an efficient technique for wireless cellular networks to transmit a large volume of common data to multiple mobile users simultaneously. To guarantee the quality of service for each mobile user in such single-hop multicasting, the base-station transmitter usually adapts its data rate to the worst channel condition among all users in a multicast group. On one hand, increasing the number of users in a multicast group leads to a more efficient utilization of spectrum bandwidth, as users in the same group can be served together. On the other hand, too many users in a group may lead to unacceptably low data rate at which the base station can transmit. Hence, a natural question that arises is how to efficiently and fairly transmit to a large number of users requiring the same message. This paper endeavors to answer this question by studying the problem of multicasting over multicarriers in wireless orthogonal frequency division multiplexing (OFDM) cellular systems. Using a unified utility maximization framework, we investigate this problem in two typical scenarios: namely, when users experience roughly equal path losses and when they experience different path losses, respectively. Through theoretical analysis, we obtain optimal multicast schemes satisfying various throughput-fairness requirements in these two cases. In particular, we show that the conventional multicast scheme is optimal in the equal-path-loss case regardless of the utility function adopted. When users experience different path losses, the group multicast scheme, which divides the users almost equally into many multicast groups and multicasts to different groups of users over nonoverlapping subcarriers, is optimal."}