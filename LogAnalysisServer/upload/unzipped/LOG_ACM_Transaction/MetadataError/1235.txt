{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/14672","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/14672","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/14673","fieldValue":" We investigate the problem of learning join queries from user examples. The user is presented with a set of candidate tuples and is asked to label them as positive or negative examples, depending on whether or not she would like the tuples as part of the join result. The goal is to quickly infer an arbitrary n-ary join predicate across an arbitrary number m of relations while keeping the number of user interactions as minimal as possible. We assume no prior knowledge of the integrity constraints across the involved relations. Inferring the join predicate across multiple relations when the referential constraints are unknown may occur in several applications, such as data integration, reverse engineering of database queries, and schema inference. In such scenarios, the number of tuples involved in the join is typically large. We introduce a set of strategies that let us inspect the search space and aggressively prune what we call uninformative tuples, and we directly present to the user the informative ones‚\u20AC\u201Dthat is, those that allow the user to quickly find the goal query she has in mind. In this article, we focus on the inference of joins with equality predicates and also allow disjunctive join predicates and projection in the queries. We precisely characterize the frontier between tractability and intractability for the following problems of interest in these settings: consistency checking, learnability, and deciding the informativeness of a tuple. Next, we propose several strategies for presenting tuples to the user in a given order that allows minimization of the number of interactions. We show the efficiency of our approach through an experimental study on both benchmark and synthetic datasets."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/14673","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/14673","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/2396","fieldValue":" Let G &equals; (V,E) be an undirected graph and let S ‚ä\u2020 V. The S-connectivity $$Œª^S_G(u,v) of a node pair (u,v) in G is the maximum number of uv-paths that no two of them have an edge or a node in S ‚à\u2019 {u,v} in common. The corresponding Connectivity Augmentation (CA) problem is: given a graph G &equals; (V,E), a node subset S ‚ä\u2020 V, and a nonnegative integer requirement function r(u,v) on V √\u2014 V, add a minimum size set F of new edges to G so that ŒªSG+F(u,v) ‚\u2030• r(u,v) for all (u,v) ‚àà V √\u2014 V. Three extensively studied particular cases are: the Edge-CA (S &equals; ‚à\u2026), the Node-CA (S &equals; V), and the Element-CA (r(u,v)&equals; 0 whenever u ‚àà S or v ‚àà S). A polynomial-time algorithm for Edge-CA was developed by Frank. In this article we consider the Element-CA and the Node-CA, that are NP-hard even for r(u,v) ‚àà {0,2}. The best known ratios for these problems were: 2 for Element-CA and O(rmax ·π° ln n) for Node-CA, where rmax &equals; maxu,v$ ‚àà V r(u,v) and n &equals; &verbar;V&verbar;. Our main result is a 7\/4-approximation algorithm for the Element-CA, improving the previously best known 2-approximation. For Element-CA with r(u,v) ‚àà {0,1,2} we give a 3\/2-approximation algorithm. These approximation ratios are based on a new splitting-off theorem, which implies an improved lower bound on the number of edges needed to cover a skew-supermodular set function. For Node-CA we establish the following approximation threshold: Node-CA with r(u,v) ‚àà {0,k} cannot be approximated within O(2log1‚à\u2019&epsis; n) for any fixed &epsis; > 0, unless NP ‚ä\u2020 DTIME(npolylog(n)$)."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/2396","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/2396","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/14674","fieldValue":" The problem of finding matches of a regular expression (RE) on a string exists in many applications, such as text editing, biosequence search, and shell commands. Existing techniques first identify candidates using substrings in the RE, then verify each of them using an automaton. These techniques become inefficient when there are many candidate occurrences that need to be verified. In this article, we propose a novel technique that prunes false negatives by utilizing negative factors, which are substrings that cannot appear in an answer. A main advantage of the technique is that it can be integrated with many existing algorithms to improve their efficiency significantly. We present a detailed description of this technique. We develop an efficient algorithm that utilizes negative factors to prune candidates, then improve it by using bit operations to process negative factors in parallel. We show that negative factors, when used with necessary factors (substrings that must appear in each answer), can achieve much better pruning power. We analyze the large number of negative factors, and develop an algorithm for finding a small number of high-quality negative factors. We conducted a thorough experimental study of this technique on real datasets, including DNA sequences, proteins, and text documents, and show significant performance improvement of the state-of-the-art tools by an order of magnitude."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/14674","fieldValue":"ACM"}