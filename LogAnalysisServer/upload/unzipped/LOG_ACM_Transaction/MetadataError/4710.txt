{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3434","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3434","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25161","fieldValue":" Given the continued integration of intermittent renewable generators in electrical power grids, connection overloads are of increasing concern for grid operators. The risk of an overload due to injection variability can be described mathematically as a barrier-crossing probability of a function of a multidimensional stochastic process. Crude Monte Carlo is a well-known technique to estimate probabilities, but it may be computationally too intensive in this case as typical modern power grids rarely exhibit connection overloads. In this article, we derive an approximate rate function for the overload probability using results from large deviations theory. Based on this large deviations approximation, we apply a rare event simulation technique called splitting to estimate overload probabilities more efficiently than Crude Monte Carlo simulation. We show on example power grids with up to 11 stochastic power injections that for a fixed accuracy, Crude Monte Carlo would require tens to millions as many samples as the proposed splitting technique required. We investigate the balance between accuracy and workload of three splitting schemes, each based on a different approximation of the rate function. We justify the workload increase of large-deviation-based splitting compared to naive splittingâ\u20AC\u201Dthat is, splitting based on merely the Euclidean distance to the rare event set. For a fixed accuracy, naive splitting requires over 60 times as much CPU time as large-deviation-based splitting, illustrating its computational advantage. In these examples, naive splittingâ\u20AC\u201Dunlike large-deviation-based splittingâ\u20AC\u201Drequires even more CPU time than CMC simulation, illustrating its pitfall."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25161","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25161","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25162","fieldValue":" We develop two new online actor-critic control algorithms with adaptive feature tuning for Markov Decision Processes (MDPs). One of our algorithms is proposed for the long-run average cost objective, while the other works for discounted cost MDPs. Our actor-critic architecture incorporates parameterization both in the policy and the value function. A gradient search in the policy parameters is performed to improve the performance of the actor. The computation of the aforementioned gradient, however, requires an estimate of the value function of the policy corresponding to the current actor parameter. The value function, on the other hand, is approximated using linear function approximation and obtained from the critic. The error in approximation of the value function, however, results in suboptimal policies. In our article, we also update the features by performing a gradient descent on the Grassmannian of features to minimize a mean square Bellman error objective in order to find the best features. The aim is to obtain a good approximation of the value function and thereby ensure convergence of the actor to locally optimal policies. In order to estimate the gradient of the objective in the case of the average cost criterion, we utilize the policy gradient theorem, while in the case of the discounted cost objective, we utilize the simultaneous perturbation stochastic approximation (SPSA) scheme. We prove that our actor-critic algorithms converge to locally optimal policies. Experiments on two different settings show performance improvements resulting from our feature adaptation scheme."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25162","fieldValue":"SPSA"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25162","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25162","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25163","fieldValue":"Moon, Il-Chul"}