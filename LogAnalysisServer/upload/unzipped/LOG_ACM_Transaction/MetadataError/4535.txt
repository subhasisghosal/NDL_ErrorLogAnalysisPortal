{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/24723","fieldValue":" The one-dimensional distribution of pseudorandom numbers generated by the ratio of uniforms method using linear congruential generators (LCGs) as the source of uniform random number is investigated in this note. Due to the two-dimensional lattice structure of LCGs there is always a comparable large gap without a point in the one-dimensional distribution of any ratio of uniforms method. Lower bounds for these probabilities only depending on the modulus and the Beyer quotient of the LCG are proved for the case that Cauchy normal or exponential random numbers are generated. These bounds justify the recommendation not to use the ratio of uniform method combined with LCGs."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24723","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24723","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/24724","fieldValue":" Because dynamic memory management is an important part of a large class of computer programs, high-performance algorithms for dynamic memory management have been and will continue to be of considerable interest. The goal of this research is to explore the size and accuracy of synthetic models of program allocation behavior. These models, if accurate enough, proved an attractive alternative to algorithm evaluation based on trace-driven simulation using actual traces. Based on our analysis, we conclude that even relatively simple synthetic models can effectively emulate the allocation behavior of well-behaved programs. However, even the most complex models we investigate can only roughly approximate the behavior of more complex programs and\/or allocation policies. While synthetic models have been used to evaluate the performance of dynamic memory management algorithms, our results show that these models can be inaccurate and must be used with care. Given current trends toward more complex applications and allocation algorithms, the synthetic models we investigate are likely to be even less accurate in the future."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24724","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24724","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/24725","fieldValue":" This article deals with fast simulation techniques for estimating transient measures in highly dependable systems. The systems we consider of components with generally distributed lifetimes and repair times, with complex interaction among components. As is well known, standard simulation of highly dependable systems is very inefficient, and importance-sampling is widely used to improve efficiency. We present two new techniques, one of which is based on the uniformization approach to simulation, and the other is a natural extension of the uniformization approach which we call exponential transformation. We show that under certain assumptions, these techniques have the bounded relative error property, i.e., the relative error of the simulation estimate remains bounded as components become more and more reliable, unlike standard simulation in which it tends to infinity. This implies that only a fixed number of observations are required to achieve a given relative error, no matter how rare the failure events are."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/24725","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/24725","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/24726","fieldValue":"Lin, Yi-Bing"}