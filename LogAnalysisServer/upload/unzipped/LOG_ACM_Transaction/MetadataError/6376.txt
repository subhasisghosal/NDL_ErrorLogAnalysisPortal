{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6952","fieldValue":"GPU"}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6952","fieldValue":"SHOT"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/6952","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6952","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/6953","fieldValue":" In order to save the energy consumption of real-time embedded systems, the integration of Dynamic Voltage and Frequency Scaling (DVFS) and Device Power Management (DPM) techniques has been well studied. In this article, we propose a new energy management scheme for periodic real-time tasks with implicit deadlines. We mainly focus on the DPM part by presenting a novel approach to the real-time DPM problem. Specifically, we first identify intervals for each device, which we refer to as Crenel Intervals, by partitioning the Earliest Deadline First (EDF) schedule of the tasks that need to access the device into successive intervals. The principle for identifying Crenel Intervals is that for each task, there is only one deadline located in each Crenel Interval. Next, targeting at a single device model and a multiple device model, respectively, we propose the CI-EDF and $CI-EDF^m$ algorithms to schedule task instances in each Crenel Interval, so as to form long and continuous slacks in each Crenel Interval but without jeopardizing any task deadlines. Then, the slack in the Crenel Intervals can be utilized to perform not only DPM, but also DVFS. The experimental results show that our approaches can achieve considerably more energy savings than existing techniques with comparable quality."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/6953","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/6953","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/6954","fieldValue":"ili, eljko"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/6954","fieldValue":"Gross, Warren J"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/6954","fieldValue":" In recent years, on-chip trace generation has been recognized as a solution to the debugging of increasingly complex software. An execution trace can be seen as the most fundamentally useful type of trace, allowing the execution path of software to be determined post hoc. However, the bandwidth required to output such a trace can be excessive. Our architecture-aware trace compression (AATC) scheme adds an on-chip branch predictor and branch target buffer to reduce the volume of execution trace data in real time through on-chip compression. Novel redundancy reduction strategies are employed, most notably in exploiting the widespread use of linked branches and the compiler-driven movement of return addresses between link register, stack, and program counter. In doing so, the volume of branch target addresses is reduced by 52&percnt;, whereas other algorithmic improvements further decrease trace volume. An analysis of spatial and temporal redundancy in the trace stream allows a comparison of encoding strategies to be made for systematically increasing compression performance. A combination of differential, Fibonacci, VarLen, and Move-to-Front encodings are chosen to produce two compressor variants: a performance-focused xAATC that encodes 56.5 instructions\/bit using 24,133 gates and an area-efficient fAATC that encodes 48.1 instructions\/bit using only 9,854 gates."}