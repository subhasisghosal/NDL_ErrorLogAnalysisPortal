{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/14249","fieldValue":" The article proposes a scalable protocol for replication management in large-scale replicated systems. The protocol organizes sites and data replicas into a tree-structured, hierarchical cluster architecture. The basic idea of the protocol is to accomplish the complex task of updating replicated data with a very large number of replicas by a set of related but independently committed transactions. Each transaction is responsible for updating replicas in exactly one cluster and invoking additional transactions for member clusters. Primary copies (one from each cluster) are updated by a cross-cluster transaction. Then each cluster is independently updated by a separate transaction. This decoupled update propagation process results in possible multiple views of replicated data in a cluster. Compared to other replicated data management protocols, the proposed protocol has several unique advantages. First, thanks to a smaller number of replicas each transaction needs to atomically update in a cluster, the protocol significantly reduces the transaction abort rate, which tends to soar in large transactional systems. Second, the protocol improves user-level transaction response time as top-level update transactions are allowed to commit before all replicas have been updated. Third, read-only queries have the flexibility to see database views of different degrees of consistency and data currency. This ranges from global, most up to date, and consistent views, to local, consistent, but potentially old views, to local, nearest to users but potentially inconsistent views. Fourth, the protocol maintains its scalability by allowing dynamic system reconfiguration as it grows by splitting a cluster into two or more smaller ones. Fifth, autonomy of the clusters is preserved as no specific protocol is required to update replicas within the same cluster. Clusters are, therefore, free to use any valid replication or concurrency control protocols."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/14249","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/14249","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/14250","fieldValue":" The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.In this article, we introduce a systematic approach to relaxing the serializability requirement in MDBS environments. Our approach exploits the structure of the integrity constraints and the nature of transaction programs to ensure consistency without requiring executions to be serializable. We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/14250","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/14250","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/14251","fieldValue":" Access control models, such as the ones supported by commercial DBMSs, are not yet able to fully meet many application needs. An important requirement derives from the temporal dimension that permissions have in many real-world situations. Permissions are often limited in time or may hold only for specific periods of time. In this article, we present an access control model in which periodic temporal intervals are associated with authorizations. An authorization is automatically granted in the specified intervals and revoked when such intervals expire. Deductive temporal rules with periodicity and order constraints are provided to derive new authorizations based on the presence or absence of other authorizations in specific periods of time. We provide a solution to the problem of ensuring the uniqueness of the global set of valid authorizations derivable at each instant, and we propose an algorithm to compute this set. Moreover, we address issues related to the efficiency of access control by adopting a materialization approach. The resulting model provides a high degree of flexibility and supports the specification of several protection requirements that cannot be expressed in traditional access control models."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/14251","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/14251","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/14252","fieldValue":"De Antonellis, V."}