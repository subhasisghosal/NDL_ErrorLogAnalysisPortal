{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10745","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10745","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10746","fieldValue":" Binary Exponential Backoff (BEB) is a key component of the IEEE 802.11 DCF protocol. It has been shown that BEB can achieve the theoretical limit of throughput as long as the initial backoff window size is properly selected. It, however, suffers from significant delay degradation when the network becomes saturated. It is thus of special interest for us to further design backoff schemes for IEEE 802.11 DCF networks that can achieve comparable throughput as BEB, but provide better delay performance. This paper presents a systematic study on the effect of backoff schemes on throughput and delay performance of saturated IEEE 802.11 DCF networks. In particular, a backoff scheme is defined as a sequence of backoff window sizes {Wi}. The analysis shows that a saturated IEEE 802.11 DCF network has a single steady-state operating point as long as {Wi} is a monotonic increasing sequence. The maximum throughput is found to be independent of {Wi}, yet the growth rate of {Wi} determines a fundamental tradeoff between throughput and delay performance. For illustration, Polynomial Backoff is proposed, and the effect of polynomial power x on the network performance is characterized. It is demonstrated that Polynomial Backoff with a larger is more robust against the fluctuation of the network size, but in the meanwhile suffers from a larger second moment of access delay. Quadratic Backoff (QB), i.e., Polynomial Backoff with x = 2 stands out to be a favorable option as it strikes a good balance between throughput and delay performance. The comparative study between QB and BEB confirms that QB well preserves the robust nature of BEB and achieves much better queueing performance than BEB."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/10746","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10746","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10746","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/1981","fieldValue":"Gaster, Benedict R"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1981","fieldValue":" Memory consistency models, or memory models, allow both programmers and program language implementers to reason about concurrent accesses to one or more memory locations. Memory model specifications balance the often conflicting needs for precise semantics, implementation flexibility, and ease of understanding. Toward that end, popular programming languages like Java, C, and C&plus;&plus; have adopted memory models built on the conceptual foundation of Sequential Consistency for Data-Race-Free programs (SC for DRF). These SC for DRF languages were created with general-purpose homogeneous CPU systems in mind, and all assume a single, global memory address space. Such a uniform address space is usually power and performance prohibitive in heterogeneous Systems on Chips (SoCs), and for that reason most heterogeneous languages have adopted split address spaces and operations with nonglobal visibility. There have recently been two attempts to bridge the disconnect between the CPU-centric assumptions of the SC for DRF framework and the realities of heterogeneous SoC architectures. Hower et al. proposed a class of Heterogeneous-Race-Free (HRF) memory models that provide a foundation for understanding many of the issues in heterogeneous memory models. At the same time, the Khronos Group developed the OpenCL 2.0 memory model that builds on the C&plus;&plus; memory model. The OpenCL 2.0 model includes features not addressed by HRF: primarily support for relaxed atomics and a property referred to as scope inclusion. In this article, we generalize HRF to allow formalization of and reasoning about more complicated models using OpenCL 2.0 as a point of reference. With that generalization, we (1) make the OpenCL 2.0 memory model more accessible by introducing a platform for feature comparisons to other models, (2) consider a number of shortcomings in the current OpenCL 2.0 model, and (3) propose changes that could be adopted by future OpenCL 2.0 revisions or by other, related, models."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1981","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/1981","fieldValue":"ACM"}