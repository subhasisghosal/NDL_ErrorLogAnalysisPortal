{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/12365","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/12365","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/12366","fieldValue":"St Amant, Rene"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/12366","fieldValue":" Since 2004, processor designers have increased core counts to exploit Mooreâ\u20AC™s Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area\/performance and power\/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21&percnt; of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50&percnt;. Through 2024, only 7.9Ã\u2014 average speedup is possible across commonly used parallel workloads for the topologies we study, leaving a nearly 24-fold gap from a target of doubled performance per generation."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/12366","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/12366","fieldValue":"ACM"}{"fieldName":"dc.relation.haspart","informationCode":"ERR_FORMAT_HASPART","handle":"12345678_acm\/2134","fieldValue":"[{\"visible\":false,\"sortKey\":\"September 2016\",\"expandable\":true,\"handle\":\"12345678_acm\/2183\",\"title\":\"Issue 4, September 2016\"},{\"visible\":false,\"sortKey\":\"June 2016\",\"expandable\":true,\"handle\":\"12345678_acm\/2182\",\"title\":\"Issue 3, June 2016\"},{\"visible\":false,\"sortKey\":\"February 2016\",\"expandable\":true,\"handle\":\"12345678_acm\/2180\",\"title\":\"Issue 1(Special Issue on SODA'12 and Regular Papers), February 2016\"},{\"visible\":false,\"sortKey\":\"February 2016\",\"expandable\":true,\"handle\":\"12345678_acm\/2181\",\"title\":\"Issue 2, February 2016\"},{\"visible\":false,\"sortKey\":\"December 2015\",\"expandable\":true,\"handle\":\"12345678_acm\/2179\",\"title\":\"Issue 2, December 2015\"},{\"visible\":false,\"sortKey\":\"December 2015\",\"expandable\":true,\"handle\":\"12345678_acm\/2178\",\"title\":\"Issue 1, December 2015\"}]"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/12367","fieldValue":"Wang, Edward Y"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/12367","fieldValue":" This article describes the historical context, technical challenges, and main implementation techniques used by VMware Workstation to bring virtualization to the x86 architecture in 1999. Although virtual machine monitors (VMMs) had been around for decades, they were traditionally designed as part of monolithic, single-vendor architectures with explicit support for virtualization. In contrast, the x86 architecture lacked virtualization support, and the industry around it had disaggregated into an ecosystem, with different vendors controlling the computers, CPUs, peripherals, operating systems, and applications, none of them asking for virtualization. We chose to build our solution independently of these vendors. As a result, VMware Workstation had to deal with new challenges associated with (i) the lack of virtualization support in the x86 architecture, (ii) the daunting complexity of the architecture itself, (iii) the need to support a broad combination of peripherals, and (iv) the need to offer a simple user experience within existing environments. These new challenges led us to a novel combination of well-known virtualization techniques, techniques from other domains, and new techniques. VMware Workstation combined a hosted architecture with a VMM. The hosted architecture enabled a simple user experience and offered broad hardware compatibility. Rather than exposing I\/O diversity to the virtual machines, VMware Workstation also relied on software emulation of I\/O devices. The VMM combined a trap-and-emulate direct execution engine with a system-level dynamic binary translator to efficiently virtualize the x86 architecture and support most commodity operating systems. By relying on x86 hardware segmentation as a protection mechanism, the binary translator could execute translated code at near hardware speeds. The binary translator also relied on partial evaluation and adaptive retranslation to reduce the overall overheads of virtualization. Written with the benefit of hindsight, this article shares the key lessons we learned from building the original system and from its later evolution."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/12367","fieldValue":"VMM"}