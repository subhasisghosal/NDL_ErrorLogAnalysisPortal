{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1691","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/1691","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/7609","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/7609","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/7610","fieldValue":" Considering that the existing depth recovery approaches have different limitations when applied to Kinect depth data, in this article, we propose to integrate their effective features including adaptive support region selection, reliable depth selection, and color guidance together under an optimization framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term, which are designed according to the Kinect characteristics. Our framework inherits and improves the idea of guided filtering by incorporating structure information and prior knowledge of the Kinect noise model. Through analyzing the solution to the optimization framework, we also derive a local filtering version that provides an efficient and effective way of improving the existing filtering techniques. Quantitative evaluations on our developed synthesized dataset and experiments on real Kinect data show that the proposed method achieves superior performance in terms of recovery accuracy and visual quality."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/7610","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/7610","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/7611","fieldValue":" The rapid spreading of RGB-D cameras has led to wide applications of 3D videos in both academia and industry, such as 3D entertainment and 3D visual understanding. Under these circumstances, extensive research efforts have been dedicated to RGB-D camera--oriented topics. In these topics, quality promotion of depth videos with the temporal characteristic is emerging and important. Due to the limited exposure time of RGB-D cameras, object movement can easily lead to motion blurs in intensive images, which can further result in obvious artifacts (holes or fake boundaries) in the corresponding depth frames. With regard to this problem, we propose a depth error elimination method based on time series analysis to remove the artifacts in depth images. In this method, we first locate the regions with erroneous depths in intensive images by using motion blur detection based on a time series analysis model. This is based on the fact that the depth image is calculated by intensive color images that are captured synchronously by RGB-D cameras. Then, the artifacts, such as holes or fake boundaries, are fixed by a depth error elimination method. To evaluate the performance of the proposed method, we conducted experiments on 250 images. Experimental results demonstrate that the proposed method can locate the error regions correctly and eliminate these artifacts effectively. The quality of depth video can be improved significantly by using the proposed method."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/7611","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/7611","fieldValue":"ACM"}