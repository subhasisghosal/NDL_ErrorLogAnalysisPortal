{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5618","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5619","fieldValue":" Recent years have seen an overwhelming interest in how people work together as a group. Both the nature of collaboration and research into how people collaborate is complex and multifaceted, with different research agendas, types of studies, and variations in the behavioral data collected. A better understanding of collaboration is needed in order to be able to make contributions to the design of systems to support collaboration and collaborative tasks. In this article, we combine relevant literature, past research, and a small-scale empirical study of two people individually and collaboratively constructing jigsaws. The objective is to make progress towards the goal of generating extensions to an existing task modeling approach, Task Knowledge Structures. The research described has enabled us to generate requirements for approaches to modeling collaborative tasks and also a set of requirements to be taken into account in the design of a computer-based collaborative virtual jigsaw."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/5619","fieldValue":"Towards modeling individual and collaborative construction of jigsaws using task knowledge structures (TKS)"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5619","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5619","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/1471","fieldValue":"Kushalnagar, Raja S"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/1471","fieldValue":"Lasecki, Walter S"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/1471","fieldValue":"Bigham, Jeffrey P"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1471","fieldValue":" Real-time captioning enables deaf and hard of hearing (DHH) people to follow classroom lectures and other aural speech by converting it into visual text with less than a five second delay. Keeping the delay short allows end-users to follow and participate in conversations. This article focuses on the fundamental problem that makes real-time captioning difficult: sequential keyboard typing is much slower than speaking. We first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube, such as speed and duration of speaking bursts. We then analyzed how these characteristics impact caption generation and readability, considering specifically our human-powered collaborative captioning approach. We note that most of these characteristics are also present in more general domains. For our caption comparison evaluation, we transcribed a classroom lecture in real-time using all three captioning approaches. We recruited 48 participants (24 DHH) to watch these classroom transcripts in an eye-tracking laboratory. We presented these captions in a randomized, balanced order. We show that both hearing and DHH participants preferred and followed collaborative captions better than those generated by automatic speech recognition (ASR) or professionals due to the more consistent flow of the resulting captions. These results show the potential to reliably capture speech even during sudden bursts of speed, as well as for generating â\u20ACœenhancedâ\u20AC? captions, unlike other human-powered captioning approaches."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1471","fieldValue":"ACM"}