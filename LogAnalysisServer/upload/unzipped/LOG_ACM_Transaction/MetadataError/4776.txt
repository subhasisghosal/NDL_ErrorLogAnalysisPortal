{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25379","fieldValue":" The multimodal data usually contain complementary, correlated and redundant information. Thus, multimodal fusion is useful for many multisensor applications. Here, a novel multimodal fusion algorithm is proposed, which is referred to as â\u20ACœMultiFusion.â\u20AC? The approach adopts a boosting structure where the atomic event is considered as the fusion unit. The correlation of multimodal data is used to form an overall classifier in each iteration. Moreover, by adopting the Adaboost-like structure, the overall fusion performance is improved. Both the simulation experiment and the real application show the effectiveness of the MultiFusion approach. Our approach can be applied in different multimodal applications to exploit the multimedia data characteristics and improve the performance."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25379","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25379","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3451","fieldValue":" Various algorithms have been proposed to create planar abstractions of 3D models, but there has been no systematic effort to evaluate the effectiveness of such abstractions in terms of perception of the abstracted surfaces. In this work, we perform a large crowd-sourced study involving approximately 70k samples to evaluate how well users can orient gauges on planar abstractions of commonly occurring models. We test four styles of planar abstractions against ground truth surface representations, and analyze the data to discover a wide variety of correlations between task error and measurements relating to surface-specific properties such as curvature, local thickness and medial axis distance, and abstraction-specific properties. We use these discovered correlations to create linear models to predict error in surface understanding at a given point, for both surface representations and planar abstractions. Our predictive models reveal the geometric causes most responsible for error, and we demonstrate their potential use to build upon existing planar abstraction techniques in order to improve perception of the abstracted surface."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3451","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3451","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25380","fieldValue":" In this article, we propose a novel multimedia sensor fusion approach based on heterogeneous sensors for biometric access control applications. The proposed fusion technique uses multiple acoustic and visual sensors for extracting dominant biometric cues, and combines them with nondominant cues. The performance evaluation of the proposed fusion protocol and a novel cascaded authentication approach using a 3D stereovision database shows a significant improvement in performance and robustness, with equal error rates of 42.9&percnt; (audio only), 32&percnt; (audio + 3D face + 2D lip features), 15&percnt; (audio + 3D face + 2D eye features), and 7.3&percnt; (audio-3D face + 2D lip + 2D eye-eyebrows) respectively."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25380","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25380","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25381","fieldValue":" The following article presents a novel audio-visual approach for unsupervised speaker localization in both time and space and systematically analyzes its unique properties. Using recordings from a single, low-resolution room overview camera and a single far-field microphone, a state-of-the-art audio-only speaker diarization system (speaker localization in time) is extended so that both acoustic and visual models are estimated as part of a joint unsupervised optimization problem. The speaker diarization system first automatically determines the speech regions and estimates â\u20ACœwho spoke when,â\u20AC? then, in a second step, the visual models are used to infer the location of the speakers in the video. We call this process â\u20ACœdialocalization.â\u20AC? The experiments were performed on real-world meetings using 4.5 hours of the publicly available AMI meeting corpus. The proposed system is able to exploit audio-visual integration to not only improve the accuracy of a state-of-the-art (audio-only) speaker diarization, but also adds visual speaker localization at little incremental engineering and computation costs. The combined algorithm has different properties, such as increased robustness, that cannot be observed in algorithms based on single modalities. The article describes the algorithm, presents benchmarking results, explains its properties, and systematically discusses the contributions of each modality."}