{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10702","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10703","fieldValue":" LTE's uplink (UL) efficiency critically depends on how the interference across different cells is controlled. The unique characteristics of LTE's modulation and UL resource assignment poses considerable challenges in achieving this goal because most LTE deployments have 1:1 frequency reuse, and the uplink interference can vary considerably across successive time-slots. In this paper, we propose LeAP, a measurement data-driven machine learning paradigm for power control to manage uplink interference in LTE. The data-driven approach has the inherent advantage that the solution adapts based on network traffic, propagation, and network topology, which is increasingly heterogeneous with multiple cell-overlays. LeAP system design consists of the following components: 1) design of user equipment (UE) measurement statistics that are succinct, yet expressive enough to capture the network dynamics, and 2) design of two learning-based algorithms that use the reported measurements to set the power control parameters and optimize the network performance. LeAP is standards-compliant and can be implemented in a centralized self-organized networking (SON) server resource (cloud). We perform extensive evaluations using radio network plans from a real LTE network operational in a major metro area in the US. Our results show that, compared to existing approaches, LeAP provides 4.9 Ã\u2014 gain in the 20th percentile of user data rate, 3.25 Ã\u2014 gain in median data rate."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/10703","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10703","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10703","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/10704","fieldValue":"La Porta, Tom"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/10704","fieldValue":" Traditional routing metrics designed for wireless networks are application-agnostic. In this paper, we consider a wireless network where the application flows consist of video traffic. From a user perspective, reducing the level of video distortion is critical. We ask the question \"Should the routing policies change if the end-to-end video distortion is to be minimized?\" Popular link-quality-based routing metrics (such as ETX) do not account for dependence (in terms of congestion) across the links of a path; as a result, they can cause video flows to converge onto a few paths and, thus, cause high video distortion. To account for the evolution of the video frame loss process, we construct an analytical framework to, first, understand and, second, assess the impact of the wireless network on video distortion. The framework allows us to formulate a routing policy for minimizing distortion, based on which we design a protocol for routing video traffic. We find via simulations and testbed experiments that our protocol is efficient in reducing video distortion and minimizing the user experience degradation."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/10704","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/10704","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/10704","fieldValue":"ACM"}