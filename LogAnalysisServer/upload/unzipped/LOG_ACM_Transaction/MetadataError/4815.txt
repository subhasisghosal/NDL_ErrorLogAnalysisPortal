{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25479","fieldValue":"Lin, Yu-Ru"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25479","fieldValue":" Presentation support tools, such as Microsoft PowerPoint, pose challenges both in terms of creating linear presentations from complex data and fluidly navigating such linear structures when presenting to diverse audiences. NextSlidePlease is a slideware application that addresses these challenges using a directed graph structure approach for authoring and delivering multimedia presentations. The application combines novel approaches for searching and analyzing presentation datasets, composing meaningfully structured presentations, and efficiently delivering material under a variety of time constraints. We introduce and evaluate a presentation analysis algorithm intended to simplify the process of authoring dynamic presentations, and a time management and path selection algorithm that assists users in prioritizing content during the presentation process. Results from two comparative user studies indicate that the directed graph approach promotes the creation of hyperlinks, the consideration of connections between content items, and a richer understanding of the time management consequences of including and selecting presentation material."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25479","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25479","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25480","fieldValue":" In object-based image retrieval, there are two important issues: an effective image representation method for representing image content and an effective image classification method for processing user feedback to find more images containing the user-desired object categories. In the image representation method, the local-based representation is the best selection for object-based image retrieval. As a kernel-based classification method, Support Vector Machine (SVM) has shown impressive performance on image classification. But SVM cannot work on the local-based representation unless there is an appropriate kernel. To address this problem, some representative kernels are proposed in literatures. However, these kernels cannot work effectively in object-based image retrieval due to ignoring the spatial context and the combination of local features. In this article, we present Adjacent Matrix (AM) and the Local Combined Features (LCF) to incorporate the spatial context and the combination of local features into the kernel. We propose the AM-LCF feature vector to represent image content and the AM-LCF kernel to measure the similarities between AM-LCF feature vectors. According to the detailed analysis, we show that the proposed kernel can overcome the deficiencies of existing kernels. Moreover, we evaluate the proposed kernel through experiments of object-based image retrieval on two public image sets. The experimental results show that the performance of object-based image retrieval can be improved by the proposed kernel."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25480","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/25480","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/25481","fieldValue":"Chua, Tat-Seng"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/25481","fieldValue":" Product annotation in videos is of great importance for video browsing, search, and advertisement. However, most of the existing automatic video annotation research focuses on the annotation of high-level concepts, such as events, scenes, and object categories. This article presents a novel solution to the annotation of specific products in videos by mining information from the Web. It collects a set of high-quality training data for each product by simultaneously leveraging Amazon and Google image search engine. A visual signature for each product is then built based on the bag-of-visual-words representation of the training images. A correlative sparsification approach is employed to remove noisy bins in the visual signatures. These signatures are used to annotate video frames. We conduct experiments on more than 1,000 videos and the results demonstrate the feasibility and effectiveness of our approach."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/25481","fieldValue":"ACM"}