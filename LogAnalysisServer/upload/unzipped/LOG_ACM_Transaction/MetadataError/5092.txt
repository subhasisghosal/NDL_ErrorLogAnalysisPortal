{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/3891","fieldValue":"Al Sallab, Ahmad A."}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3891","fieldValue":" The Arabic language belongs to a group of languages that require diacritization over their characters. Modern Standard Arabic (MSA) transcripts omit the diacritics, which are essential for many machine learning tasks like Text-To-Speech (TTS) systems. In this work Arabic diacritics restoration is tackled under a deep learning framework that includes the Confused Sub-set Resolution (CSR) method to improve the classification accuracy, in addition to an Arabic Part-of-Speech (PoS) tagging framework using deep neural nets. Special focus is given to syntactic diacritization, which still suffers low accuracy as indicated in prior works. Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets collected from different domains. Standard datasets like the LDC Arabic Tree Bank are used in addition to custom ones we have made available online to allow other researchers to replicate these results. Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported systems."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/3891","fieldValue":"Deep learning framework with confused sub-set resolution architecture for automatic Arabic diacritization"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3891","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3891","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3892","fieldValue":" Language models have traditionally been estimated based on relative frequencies, using count statistics that can be extracted from huge amounts of text data. More recently, it has been found that neural networks are particularly powerful at estimating probability distributions over word sequences, giving substantial improvements over state-of-the-art count models. However, the performance of neural network language models strongly depends on their architectural structure. This paper compares count models to feedforward, recurrent, and long short-term memory (LSTM) neural network variants on two large-vocabulary speech recognition tasks. We evaluate the models in terms of perplexity and word error rate, experimentally validating the strong correlation of the two quantities, which we find to hold regardless of the underlying type of the language model. Furthermore, neural networks incur an increased computational complexity compared to count models, and they differently model context dependences, often exceeding the number of words that are taken into account by count based approaches. These differences require efficient search methods for neural networks, and we analyze the potential improvements that can be obtained when applying advanced algorithms to the rescoring of word lattices on large-scale setups."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3892","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3892","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1297","fieldValue":" In this article, we complement Elinor Ostromâ\u20AC™s institutional design principles for enduring common-pool resource management with Nicholas Rescherâ\u20AC™s theory of distributive justice based on the canon of legitimate claims. Two of Ostromâ\u20AC™s principles are that the resource allocation method should be congruent with the local environment, and that those affected by the allocation method (the appropriators) should participate in its selection. However, these principles do not say anything explicitly about the fairness of the allocation method or the outcomes it produces: for this, we need a mechanism for distributive justice. Rescher identified a number of different mechanisms, each of which had both its merits and demerits, and instead maintained that distributive justice consisted in identifying the legitimate claims in context, accommodating multiple claims in case of plurality, and reconciling them in case of conflict. Accordingly, we specify a logical axiomatisation of the principles with the canon of legitimate claims, whereby a set of claims is each represented as a voting function, which collectively determine the rank order in which resources are allocated. The appropriators vote on the weight attached to the scoring functions, and so self-organise the allocation method, taking into account both the plurality of and conflict between the claims. Therefore, the appropriators exercise collective choice over the method, and the method itself is congruent with the local environment, taking into account both the resources available and the relative claims of the appropriators. Experiments with a variant of the linear public good game show that this pluralistic self-organising approach produces a better balance of utility and fairness (for agents that comply with the rules of the game) compared to monistic or fixed approaches, provide â\u20ACœfairness over timeâ\u20AC? (a series of ostensibly unfair individual allocations is revealed to be cumulatively fair), and offer an intuition of how to resolve the free-rider phenomenon in provision and appropriation of common-pool resources."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1297","fieldValue":"ACM"}