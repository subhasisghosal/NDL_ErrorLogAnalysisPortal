{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/7176","fieldValue":" Recommender systems increasingly use contextual and demographical data as a basis for recommendations. Users, however, often feel uncomfortable providing such information. In a privacy-minded design of recommenders, users are free to decide for themselves what data they want to disclose about themselves. But this decision is often complex and burdensome, because the consequences of disclosing personal information are uncertain or even unknown. Although a number of researchers have tried to analyze and facilitate such information disclosure decisions, their research results are fragmented, and they often do not hold up well across studies. This article describes a unified approach to privacy decision research that describes the cognitive processes involved in usersâ\u20AC™ â\u20ACœprivacy calculusâ\u20AC? in terms of system-related perceptions and experiences that act as mediating factors to information disclosure. The approach is applied in an online experiment with 493 participants using a mock-up of a context-aware recommender system. Analyzing the results with a structural linear model, we demonstrate that personal privacy concerns and disclosure justification messages affect the perception of and experience with a system, which in turn drive information disclosure decisions. Overall, disclosure justification messages do not increase disclosure. Although they are perceived to be valuable, they decrease usersâ\u20AC™ trust and satisfaction. Another result is that manipulating the order of the requests increases the disclosure of items requested early but decreases the disclosure of items requested later."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/7176","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/7176","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/7177","fieldValue":"Bekris, Kostas E"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/7177","fieldValue":" Indoor localization and navigation systems for individuals with Visual Impairments (VIs) typically rely upon extensive augmentation of the physical space, significant computational resources, or heavy and expensive sensors; thus, few systems have been implemented on a large scale. This work describes a system able to guide people with VIs through indoor environments using inexpensive sensors, such as accelerometers and compasses, which are available in portable devices like smart phones. The method takes advantage of feedback from the human user, who confirms the presence of landmarks, something that users with VIs already do when navigating in a building. The system calculates the user's location in real time and uses it to provide audio instructions on how to reach the desired destination. Initial early experiments suggested that the accuracy of the localization depends on the type of directions and the availability of an appropriate transition model for the user. A critical parameter for the transition model is the user's step length. Consequently, this work also investigates different schemes for automatically computing the user's step length and reducing the dependence of the approach on the definition of an accurate transition model. In this way, the direction provision method is able to use the localization estimate and adapt to failed executions of paths by the users. Experiments are presented that evaluate the accuracy of the overall integrated system, which is executed online on a smart phone. Both people with VIs and blindfolded sighted people participated in the experiments, which included paths along multiple floors that required the use of stairs and elevators."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/7177","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/7177","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/7178","fieldValue":" This article presents Gesture Interaction DEsigner (GIDE), an innovative application for gesture recognition. Instead of recognizing gestures only after they have been entirely completed, as happens in classic gesture recognition systems, GIDE exploits the full potential of gestural interaction by tracking gestures continuously and synchronously, allowing users to both control the target application moment to moment and also receive immediate and synchronous feedback about system recognition states. By this means, they quickly learn how to interact with the system in order to develop better performances. Furthermore, rather than learning the predefined gestures of others, GIDE allows users to design their own gestures, making interaction more natural and also allowing the applications to be tailored by users' specific needs. We describe our system that demonstrates these new qualitiesâ\u20AC\u201Dthat combine to provide fluid gesture interaction designâ\u20AC\u201Dthrough evaluations with a range of performers and artists."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/7178","fieldValue":"Fluid gesture interaction design: Applications of continuous recognition for the design of modern gestural interfaces"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/7178","fieldValue":"ACM"}