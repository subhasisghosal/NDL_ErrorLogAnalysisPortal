{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/3595","fieldValue":"Temporally Varying Weight Regression: A Semi-Parametric Trajectory Model for Automatic Speech Recognition"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3595","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3595","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3596","fieldValue":" This paper presents a family of discriminative manifold learning approaches to feature space dimensionality reduction in noise robust automatic speech recognition (ASR). The specific goal of these techniques is to preserve local manifold structure in feature space while at the same time maximizing the separability between classes of feature vectors. In the manifold space, the relationships among the feature vectors are defined using nonlinear kernels. Two separate distance measures are used to characterize the kernels, namely the conventional Euclidean distance and a cosine-correlation based distance. The performance of the proposed techniques is evaluated on two task domains involving noise corrupted utterances of connected digits and read newspaper text. Performance is compared to existing approaches used for feature space transformations, including linear discriminant analysis (LDA) and locality preserving linear projections (LPP). The proposed approaches are found to provide a significant reduction in word error rate (WER) with respect to the more well-known techniques for a variety of noise conditions. Another contribution of the paper is to quantify the interaction between acoustic noise conditions and the shape and size of local neighborhoods which are used in manifold learning to define local relationships among feature vectors. Based on this analysis, a procedure for reducing the impact of varying acoustic conditions on manifold learning is proposed ."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3596","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3596","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3597","fieldValue":" In this paper, we present novel speaking-aid systems based on one-to-many eigenvoice conversion (EVC) to enhance three types of alaryngeal speech: esophageal speech, electrolaryngeal speech, and body-conducted silent electrolaryngeal speech. Although alaryngeal speech allows laryngectomees to utter speech sounds, it suffers from the lack of speech quality and speaker individuality. To improve the speech quality of alaryngeal speech, alaryngeal-speech-to-speech (AL-to-Speech) methods based on statistical voice conversion have been proposed. In this paper, one-to-many EVC capable of flexibly controlling the converted voice quality by adapting the conversion model to given target natural voices is further implemented for the AL-to-Speech methods to effectively recover speaker individuality of each type of alaryngeal speech. These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems are capable of effectively addressing the issues of alaryngeal speech, e.g., yielding significant improvements in speech quality of each type of alaryngeal speech."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3597","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3597","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3598","fieldValue":" Neural network language models (NNLMs) have achieved very good performance in large-vocabulary continuous speech recognition (LVCSR) systems. Because decoding with NNLMs is computationally expensive, there is interest in developing methods to approximate NNLMs with simpler language models that are suitable for fast decoding. In this work, we propose an approximate method for converting a feedforward NNLM into a back-off n-gram language model that can be used directly in existing LVCSR decoders. We convert NNLMs of increasing order to pruned back-off language models, using lower-order models to constrain the n-grams allowed in higher-order models. In experiments on Broadcast News data, we find that the resulting back-off models retain the bulk of the gain achieved by NNLMs over conventional n-gram language models, and give accuracy improvements as compared to existing methods for converting NNLMs to back-off models. In addition, the proposed approach can be applied to any type of non-back-off language model to enable efficient decoding."}