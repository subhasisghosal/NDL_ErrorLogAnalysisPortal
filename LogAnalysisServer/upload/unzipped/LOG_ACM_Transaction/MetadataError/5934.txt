{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5752","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5752","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1483","fieldValue":" People using wheelchairs have access to fewer sports and other physically stimulating leisure activities than nondisabled persons, and often lead sedentary lifestyles that negatively influence their health. While motion-based video games have demonstrated great potential of encouraging physical activity among nondisabled players, the accessibility of motion-based games is limited for persons with mobility disabilities, thus also limiting access to the potential health benefits of playing these games. In our work, we address this issue through the design of wheelchair-accessible motion-based game controls. We present $KINECT^Wheels, a toolkit designed to integrate wheelchair movements into motion-based games. Building on the toolkit, we developed Cupcake Heaven, a wheelchair-based video game designed for older adults using wheelchairs, and we created Wheelchair Revolution, a motion-based dance game that is accessible to both persons using wheelchairs and nondisabled players. Evaluation results show that KINECTWheels$ can be applied to make motion-based games wheelchair-accessible, and that wheelchair-based games engage broad audiences in physically stimulating play. Through the application of the wheelchair as an enabling technology in games, our work has the potential of encouraging players of all ages to develop a positive relationship with their wheelchair."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1483","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/1483","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5753","fieldValue":" Many interactive systems require users to navigate through large sets of data and commands using constrained input devicesâ\u20AC\u201Dsuch as scroll rings, rocker switches, or specialized keypadsâ\u20AC\u201Dthat provide less power and flexibility than traditional input devices like mice or touch screens. While performance with more traditional devices has been extensively studied in human-computer interaction, there has been relatively little investigation of human performance with constrained input. As a result, there is little understanding of what factors govern performance in these situations, and how interfaces should be designed to optimize interface actions such as navigation and selection. Since constrained input is now common in a wide variety of interactive systems (such as mobile phones, audio players, in-car navigation systems, and kiosk displays), it is important for designers to understand what factors affect performance. To aid in this understanding, we present the Constrained Input Navigation (CIN) model, a predictive model that allows accurate determination of human navigation and selection performance in constrained-input scenarios. CIN identifies three factors that underlie user efficiency: the performance of the interface type for single-level item selection (where interface type depends on the input and output devices, the interactive behavior, and the data organization), the hierarchical structure of the information space, and the user's experience with the items to be selected. We show through experiments that, after empirical calibration, the model's predictions fit empirical data well, and discuss why and how each of the factors affects performance. Models like CIN can provide valuable theoretical and practical benefits to designers of constrained-input systems, allowing them to explore and compare a much wider variety of alternate interface designs without the need for extensive user studies."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5753","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/5753","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/5754","fieldValue":" A lot of research has been done within the area of mobile computing and context-awareness over the last 15 years, and the idea of systems adapting to their context has produced promising results for overcoming some of the challenges of user interaction with mobile devices within various specialized domains. However, today it is still the case that only a limited body of theoretically grounded knowledge exists that can explain the relationship between users, mobile system user interfaces, and their context. Lack of such knowledge limits our ability to elevate learning from the mobile systems we develop and study from a concrete to an abstract level. Consequently, the research field is impeded in its ability to leap forward and is limited to incremental steps from one design to the next. Addressing the problem of this void, this article contributes to the body of knowledge about mobile interaction design by promoting a theoretical approach for describing and understanding the relationship between user interface representations and user context. Specifically, we promote the concept of indexicality derived from semiotics as an analytical concept that can be used to describe and understand a design. We illustrate the value of the indexicality concept through an analysis of empirical data from evaluations of three prototype systems in use. Based on our analytical and empirical work we promote the view that users interpret information in a mobile computer user interface through creation of meaningful indexical signs based on the ensemble of context and system."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/5754","fieldValue":"ACM"}