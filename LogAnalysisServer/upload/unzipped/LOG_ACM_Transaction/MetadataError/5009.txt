{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3698","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3698","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3699","fieldValue":" Mitigating the effects of reverberation is a significant challenge for real-world spatial soundfield reproduction, but the necessity of a large number of reproduction channels increases the complexity and presents several challenges to existing listening room compensation techniques. In this paper, we present an adaptive room compensation method to overcome the effects of reverberation within a region, using a model description of the reverberant soundfield. We propose the reverberant channel estimation and compensation be carried out in a single step using completely decoupled adaptive filters; thus, reducing the complexity of the overall process. We compare the soundfield reproduction performance with existing adaptive and nonadaptive room compensation methods through several simulation examples. The performance of the proposed method is comparable to existing techniques, and achieves a normalized wideband region reproduction error of 1% at a signal-to-noise ratio of 50 dB, within a 1 m radius region of interest using 60 loudspeakers and 55 microphones at frequencies below 1 kHz. Robust behavior of the room compensator is demonstrated down to direct-to-reverberant-path power ratios of -5 dB. Overall, the results suggest that the proposed method can diagonalize the room compensation system, leading to amore robust and parallel implementation for spatial soundfield reproduction."}{"fieldName":"dc.title","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/3699","fieldValue":"Efficient multi-channel adaptive room compensation for spatial soundfield reproduction using a modal decomposition"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3699","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3699","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/3700","fieldValue":"Abdel-Hamid, Ossama"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/3700","fieldValue":"Mohamed, Abdel-Rahman"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3700","fieldValue":" Recently, the hybrid deep neural network (DNN)- hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6%-10% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3700","fieldValue":"ACM"}