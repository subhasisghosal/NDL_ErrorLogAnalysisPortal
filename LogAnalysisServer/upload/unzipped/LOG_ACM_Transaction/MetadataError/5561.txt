{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/4863","fieldValue":" In recent years a particular class of probabilistic graphical modelsâ\u20AC\u201Dcalled topic modelsâ\u20AC\u201Dhas proven to represent an useful and interpretable tool for understanding and mining microarray data. In this context, such models have been almost only applied in the clustering scenario, whereas the classification task has been disregarded by researchers. In this paper, we thoroughly investigate the use of topic models for classification of microarray data, starting from ideas proposed in other fields (e.g., computer vision). A classification scheme is proposed, based on highly interpretable features extracted from topic models, resulting in a hybrid generative-discriminative approach; an extensive experimental evaluation, involving 10 different literature benchmarks, confirms the suitability of the topic models for classifying expression microarray data."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/4863","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.subject","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/4863","fieldValue":"Biological system modeling,Data models,Computational modeling,Probabilistic logic,Feature extraction,Analytical models,Context,hybrid generative discriminative approaches,Expression microarray,topic models"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/4863","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/4863","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/4864","fieldValue":"Di Gregorio, Federico"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/4864","fieldValue":" Nowadays storing data derived from deep sequencing experiments has become pivotal and standard compression algorithms do not exploit in a satisfying manner their structure. A number of reference-based compression algorithms have been developed but they are less adequate when approaching new species without fully sequenced genomes or nongenomic data. We developed a tool that takes advantages of fastq characteristics and encodes them in a binary format optimized in order to be further compressed with standard tools (such as gzip or lzma). The algorithm is straightforward and does not need any external reference file, it scans the fastq only once and has a constant memory requirement. Moreover, we added the possibility to perform lossy compression, losing some of the original information (IDs and\/or qualities) but resulting in smaller files; it is also possible to define a quality cutoff under which corresponding base calls are converted to N. We achieve 2.82 to 7.77 compression ratios on various fastq files without losing information and 5.37 to 8.77 losing IDs, which are often not used in common analysis pipelines. In this paper, we compare the algorithm performance with known tools, usually obtaining higher compression levels."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/4864","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.subject","informationCode":"WARN_TEXT_LENGTH_LARGE","handle":"12345678_acm\/4864","fieldValue":"Encoding,Bioinformatics,Genomics,Decoding,Standards,Compression algorithms,IEEE transactions,algorithms for data and knowledge management,Biology and genetics"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/4864","fieldValue":"ACM"}