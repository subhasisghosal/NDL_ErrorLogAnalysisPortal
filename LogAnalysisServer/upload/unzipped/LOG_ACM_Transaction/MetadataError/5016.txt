{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/3714","fieldValue":"Jayendra-Lakshman, Mythreya"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3714","fieldValue":" The MPEG committee has completed development of a new audio coding standard called \"MPEG-4 advanced audio coding-enhanced low delay\" (AAC-ELD). AAC-ELD uses low delay spectral band replication (LD-SBR) technology together with a low delay time domain alias cancellation (LD TDAC) filterbank in the encoder to achieve both high coding efficiency and low algorithmic delay. In this paper, we present fast algorithms for implementing LD-TDAC filterbanks in AAC-ELD. Two types of fast algorithms are presented. In the first, we map LD-TDAC analysis and synthesis filterbanks to modified discrete cosine transform (MDCT) and inverse modified discrete cosine transform (IMDCT), respectively. Since MDCT\/IMDCT are already extensively used in AAC and they have many fast algorithms, this mapping not only provides a fast implementation but also allows a common implementation of the filterbanks in AAC Low Complexity (AAC-LC), AAC Low Delay (AAC-LD) and AAC-ELD codecs. In the second algorithm, we provide a mapping to discrete Cosine transform of type II. The mapping to DCT-II allows the merger of the matrix operations with the windowing stage that precedes or follows them. This further reduces the number of multiplications and leads to an algorithm with the lowest known arithmetic complexity. For filterbanks of lengths 1024 and 960, we also present a new fast factorization of 15-point DCT-II that requires only 14 irrational multiplications, 3 dyadic rational multiplications and 67 additions."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3714","fieldValue":"AAC"}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3714","fieldValue":"DCT"}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3714","fieldValue":"MDCT"}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3714","fieldValue":"MPEG"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/3714","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/3714","fieldValue":"ACM"}{"fieldName":"dc.contributor.author","informationCode":"WARN_INVALID_PERSON","handle":"12345678_acm\/3715","fieldValue":"Abdel-Hamid, Ossama"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/3715","fieldValue":" Fast adaptation of deep neural networks (DNN) is an important research topic in deep learning. In this paper, we have proposed a general adaptation scheme for DNN based on discriminant condition codes, which are directly fed to various layers of a pre-trained DNN through a new set of connection weights. Moreover, we present several training methods to learn connection weights from training data as well as the corresponding adaptation methods to learn new condition code from adaptation data for each new test condition. In this work, the fast adaptation scheme is applied to supervised speaker adaptation in speech recognition based on either frame-level cross-entropy or sequence-level maximum mutual information training criterion. We have proposed three different ways to apply this adaptation scheme based on the so-called speaker codes: i) Nonlinear feature normalization in feature space; ii) Direct model adaptation of DNN based on speaker codes; iii) Joint speaker adaptive training with speaker codes. We have evaluated the proposed adaptation methods in two standard speech recognition tasks, namely TIMIT phone recognition and large vocabulary speech recognition in the Switchboard task. Experimental results have shown that all three methods are quite effective to adapt large DNN models using only a small amount of adaptation data. For example, the Switchboard results have shown that the proposed speaker-code-based adaptation methods may achieve up to 8-10% relative error reduction using only a few dozens of adaptation utterances per speaker. Finally, we have achieved very good performance in Switchboard (12.1% in WER) after speaker adaptation using sequence training criterion, which is very close to the best performance reported in this task (\"Deep convolutional neural networks for LVCSR,\" T. N. Sainath et al., Proc. IEEE Acoust., Speech, Signal Process., 2013)."}