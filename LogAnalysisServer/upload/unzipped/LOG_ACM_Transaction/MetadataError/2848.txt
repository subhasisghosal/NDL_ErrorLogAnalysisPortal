{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/19353","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19353","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/19354","fieldValue":" Dantzig--Wolfe Decomposition is recognized as a powerful, algorithmic tool for solving linear programs of block-angular form. While use of the approach has been reported in a wide variety of domains, there has not been a general implementation of Dantzig--Wolfe decomposition available. This article describes an open-source implementation of the algorithm. It is general in the sense that any properly decomposed linear program can be provided to the software for solving. While the original description of the algorithm was motivated by its reduced memory usage, modern computers can also take advantage of the algorithm's inherent parallelism. This implementation is parallel and built upon the POSIX threads (pthreads) library. Some computational results are provided to motivate use of such parallel solvers, as this implementation outperforms state-of-the-art commercial solvers in terms of wall-clock runtime by an order of magnitude or more on several problem instances."}{"fieldName":"dc.identifier.other","informationCode":"ERR_FORMAT_DOI","handle":"12345678_acm\/19354","fieldValue":"{\"doi\":\"http:\/\/dx.doi.org\/10.1145\/2450153.2450159\"}"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/19354","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19354","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/19355","fieldValue":" In LAPACK many matrix operations are cast as block algorithms which iteratively process a panel using an unblocked algorithm and then update a remainder matrix using the high performance Level 3 BLAS. The Level 3 BLAS have excellent scaling, but panel processing tends to be bus bound, and thus scales with bus speed rather than the number of processors (p). Amdahl's law therefore ensures that as p grows, the panel computation will become the dominant cost of these LAPACK routines. Our contribution is a novel parallel cache assignment approach to the panel factorization which we show scales well with p. We apply this general approach to the QR, QL, RQ, LQ and LU panel factorizations. We show results for two commodity platforms: an 8-core Intel platform and a 32-core AMD platform. For both platforms and all twenty implementations (five factorizations each of which is available in 4 types), we present results that demonstrate that our approach yields significant speedup over the existing state of the art."}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19355","fieldValue":"ATLAS"}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/19355","fieldValue":"LAPACK"}{"fieldName":"dc.subject","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/19355","fieldValue":"LU"}