{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/9613","fieldValue":" In this paper, we study the gains from opportunistic spectrum usage when neither sender or receiver are aware of the current channel conditions in different frequency bands. Hence to select the best band for sending data, nodes first need to measure the channel in different bands which takes time away from sending actual data. We analyze the gains from opportunistic band selection by deriving an optimal skipping rule, which balances the throughput gain from finding a good quality band with the overhead of measuring multiple bands. We show that opportunistic band skipping is most beneficial in low signal to noise scenarios, which are typically the cases when the node throughput in single-band (no opportunism) system is the minimum. To study the impact of opportunism on network throughput, we devise a CSMA\/CA protocol, Multi-band Opportunistic Auto Rate (MOAR), which implements the proposed skipping rule on a per node pair basis. The proposed protocol exploits both time and frequency diversity, and is shown to result in typical throughput gains of 20% or more over a protocol which only exploits time diversity, Opportunistic Auto Rate (OAR)."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/9613","fieldValue":"{\"eissn\":\"\"}"}{"fieldName":"dc.subject","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/9613","fieldValue":"CSMA\/CA"}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/9613","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/9613","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/1868","fieldValue":" First-level caches are usually split for both instructions and data instead of unifying them in a single cache. Although that approach eases the pipeline design and provides a simple way to independently treat data and instructions, its global hit rate is usually smaller than that of a unified cache. Furthermore, unified lower-level caches usually behave and process memory requests disregarding whether they are data or instruction requests. In this article, we propose a new technique aimed to balance the amount of space devoted to instructions and data for optimizing set-associative caches: the Virtually Split Cache or VSC. Our technique combines the sharing of resources from unified approaches with the bandwidth and parallelism that split configurations provide, thus reducing power consumption while not degrading performance. Our design dynamically adjusts cache resources devoted to instructions and data depending on their particular demand. Two VSC designs are proposed in order to track the instructions and data requirements. The Shadow Tag VSC (ST-VSC) is based on shadow tags that store the last evicted line related to data and instructions in order to determine how well the cache would work with one more way per set devoted to each kind. The Global Selector VSC (GS-VSC) uses a saturation counter that is updated every time a cache miss occurs either under an instruction or data request applying a duel-like mechanism. Experiments with a variable and a fixed latency VSC show that ST-VSC and GS-VSC reduce on average the cache hierarchy power consumption by 29&percnt; and 24&percnt;, respectively, with respect to a standard baseline. As for performance, while the fixed latency designs virtually match the split baseline in a single-core system, a variable latency ST-VSC and GS-VSC increase the average IPC by 2.5&percnt; and 2&percnt;, respectively. In multicore systems, even the slower fixed latency ST-VSC and GS-VSC designs improve the baseline IPC by 3.1&percnt; and 2.5&percnt;, respectively, in a four-core system thanks to the reduction in the bandwidth demanded from the lower cache levels. This is in contrast with many techniques that trade performance degradation for power consumption reduction. VSC particularly benefits embedded processors with a single level of cache, where up to an average 9.2&percnt; IPC improvement is achieved. Interestingly, we also find that partitioning the LLC for instructions and data can improve performance around 2&percnt;."}{"fieldName":"dc.publisher","informationCode":"WARN_TEXT_LENGTH_SMALL","handle":"12345678_acm\/1868","fieldValue":"ACM"}{"fieldName":"dc.publisher","informationCode":"WARN_ALL_WORD_UPPER","handle":"12345678_acm\/1868","fieldValue":"ACM"}{"fieldName":"dc.description.abstract","informationCode":"ERR_SPACE_AT_EDGE","handle":"12345678_acm\/9614","fieldValue":" Smart antennas represent a broad variety of antennas that differ in their performance and transceiver complexity. The superior capabilities of smart antennas, however, can be leveraged only through appropriately designed higher layer network protocols, including at the medium access control (MAC) layer. Although several related works have considered such tailored protocols, they do so in the context of specific antenna technologies. In this paper, we explore the possibility for a unified approach to medium access control in ad hoc networks with smart antennas. We first present a unified representation of the PHY layer capabilities of the different types of smart antennas, and their relevance to MAC layer design. We then define a unified MAC problem formulation, and derive unified MAC algorithms (both centralized and distributed) from the formulation. Finally, using the algorithms developed, we investigate the relative performance trade-offs of the different technologies under varying network conditions. We also analyze theoretically the performance bounds of the different smart antenna technologies when the available gains are exploited for rate increase and communication range increase."}{"fieldName":"dc.identifier.other","informationCode":"ERR_NULL_VALUE","handle":"12345678_acm\/9614","fieldValue":"{\"eissn\":\"\"}"}